{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtstudents/anaconda3/envs/zhen_gpu/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using TensorFlow backend.\n",
      "In /home/jtstudents/anaconda3/envs/zhen_gpu/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The text.latex.preview rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/jtstudents/anaconda3/envs/zhen_gpu/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The mathtext.fallback_to_cm rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/jtstudents/anaconda3/envs/zhen_gpu/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: Support for setting the 'mathtext.fallback_to_cm' rcParam is deprecated since 3.3 and will be removed two minor releases later; use 'mathtext.fallback : 'cm' instead.\n",
      "In /home/jtstudents/anaconda3/envs/zhen_gpu/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The validate_bool_maybe_none function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/jtstudents/anaconda3/envs/zhen_gpu/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/jtstudents/anaconda3/envs/zhen_gpu/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The keymap.all_axes rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/jtstudents/anaconda3/envs/zhen_gpu/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_path rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/jtstudents/anaconda3/envs/zhen_gpu/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_args rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n"
     ]
    }
   ],
   "source": [
    "import os, time, random, logging\n",
    "import cv2\n",
    "import numpy as np\n",
    "#import mtcnn\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "print(tf.version.VERSION)\n",
    "\n",
    "from tensorflow import keras\n",
    "#from keras import models, layers, backend as K\n",
    "#from keras.layers import Activation\n",
    "#from keras.callbacks import ModelCheckpoint\n",
    "##from tensorflow.keras.utils import get_custom_objects\n",
    "#from keras.utils.generic_utils import get_custom_objects\n",
    "from tqdm.keras import TqdmCallback\n",
    "\n",
    "from IPython import display\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" PATH VAR \"\"\"\n",
    "\n",
    "base_vigia_dir = \"/media/jtstudents/HDD/.zuble/vigia\"\n",
    "server_trainame_folder = '/media/jtstudents/HDD/.zuble/xdviol/train'\n",
    "server_testname_folder = '/media/jtstudents/HDD/.zuble/xdviol/test'\n",
    "\n",
    "#base_vigia_dir = \"/raid/DATASETS/.zuble/vigia\"\n",
    "#server_trainame_folder = '/raid/DATASETS/anomaly/XD_Violence/training/'\n",
    "#server_testname_folder = '/raid/DATASETS/anomaly/XD_Violence/testing'\n",
    "##server_trainame_folder = '/home/zhen/Documents/Remote/raid/DATASETS/anomaly/UCF_Crimes/Videos'\n",
    "\n",
    "model_path = base_vigia_dir+'/zu++/model/model/'\n",
    "ckpt_path = base_vigia_dir+'/zu++/model/ckpt/'\n",
    "hist_path = base_vigia_dir+'/zu++/model/hist/'\n",
    "rslt_path_zu = base_vigia_dir+'/zu++/model/rslt/'\n",
    "rslt_path_zhen = base_vigia_dir+'/zhen++/parameters_results'\n",
    "\n",
    "weights_path_zu = base_vigia_dir+'/zu++/model/weights/'\n",
    "weights_path_zhen = base_vigia_dir+\"/zhen++/parameters_saved\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TEST/TRAIN FILES \"\"\"\n",
    "def test_files(onev = 0):\n",
    "    \"\"\"\n",
    "    GENERATE LIST of TEST FILES\n",
    "    \"\"\"\n",
    "    test_fn, test_normal_fn, test_abnormal_fn = [],[],[]\n",
    "    y_test_labels, y_test_norm, y_test_abnor = [],[],[]\n",
    "    \n",
    "    #all test files\n",
    "    if onev == 0:\n",
    "        for root, dirs, files in os.walk(server_testname_folder):\n",
    "            for file in files:\n",
    "                if file.find('.mp4') != -1:\n",
    "                    if 'label_A' not in file:\n",
    "                        test_normal_fn.append(os.path.join(root, file))\n",
    "                        y_test_norm.append(1)\n",
    "                    else:\n",
    "                        test_abnormal_fn.append(os.path.join(root, file))\n",
    "                        y_test_abnor.append(0)\n",
    "                        \n",
    "        test_fn = test_normal_fn + test_abnormal_fn\n",
    "        y_test_labels = y_test_norm + y_test_abnor\n",
    "    #only onev random files\n",
    "    else :\n",
    "        test_abn_fn = [x for x in os.listdir(server_testname_folder) if 'label_A' not in x]\n",
    "        test_nor_fn = [x for x in os.listdir(server_testname_folder) if 'label_A' in x]\n",
    "        \n",
    "        onev_abnor = int(onev/2)\n",
    "        while True: \n",
    "            ap = random.choice(test_abn_fn) \n",
    "            if ap not in test_fn: \n",
    "                test_fn.append(server_testname_folder+\"/\"+ap)\n",
    "                y_test_labels.append(1)\n",
    "                if len(test_fn) == onev_abnor: \n",
    "                    break \n",
    "        while True: \n",
    "            ap = random.choice(test_nor_fn) \n",
    "            if ap not in test_fn: \n",
    "                test_fn.append(server_testname_folder+\"/\"+ap)\n",
    "                y_test_labels.append(0)\n",
    "                if len(test_fn) == onev: \n",
    "                    break    \n",
    "    \n",
    "    print(\"\\ntest_fn\",np.shape(test_fn),\"\\ntest_normal_fn\",np.shape(test_normal_fn),\"\\ntest_abnormal_fn\",np.shape(test_abnormal_fn))\n",
    "    print(\"\\ny_test_labels\",np.shape(y_test_labels),\"\\ny_test_norm\",np.shape(y_test_norm),\"\\ny_test_abnor\",np.shape(y_test_abnor))\n",
    "    \n",
    "    return test_fn, y_test_labels\n",
    "\n",
    "def train_files():\n",
    "    \"\"\"\n",
    "    GENERATING LIST of TRAIN FILES\n",
    "    \"\"\"\n",
    "    train_fn = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(server_trainame_folder):\n",
    "        for file in files:\n",
    "            if file.find('.mp4') != -1:\n",
    "                train_fn.append(os.path.join(root, file))\n",
    "                \n",
    "    print(\"\\ntrain_fn=\",np.shape(train_fn))\n",
    "    \n",
    "    return train_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "test_fn (800,) \n",
      "test_normal_fn (500,) \n",
      "test_abnormal_fn (300,)\n",
      "\n",
      "y_test_labels (800,) \n",
      "y_test_norm (500,) \n",
      "y_test_abnor (300,)\n",
      "\n",
      "train_fn= (3954,)\n",
      "\n",
      "time_str= 1671116581.55242 \n",
      "\n",
      "\n",
      "\n",
      "\tBATCH TYPE 1\n"
     ]
    }
   ],
   "source": [
    "\"\"\" GLOBAL VAR + test/train inputs \"\"\"\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "target_height = 120\n",
    "target_width = 160\n",
    "frame_max = 4000\n",
    "\n",
    "test_fn, y_test_labels = test_files()\n",
    "train_fn = train_files()\n",
    "\n",
    "update_index = range(0, len(train_fn))\n",
    "time_str = str(time.time())\n",
    "print(\"\\ntime_str=\",time_str,\"\\n\")\n",
    "\n",
    "\n",
    "batch_type = 1  # =1 last batch has 4000 frames // =2 last batch has no repetead frames\n",
    "print(\"\\n\\n\\tBATCH TYPE\",batch_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" INPUT DATA\"\"\"\n",
    "def input_video_data(file_name):\n",
    "    print(\"\\n\\nINPUT_VIDEO_DATA\\n\")\n",
    "    #file_name = 'C:\\\\Bosch\\\\Anomaly\\\\training\\\\videos\\\\13_007.avi'\n",
    "    #file_name = '/raid/DATASETS/anomaly/UCF_Crimes/Videos/Training_Normal_Videos_Anomaly/Normal_Videos308_x264.mp4'\n",
    "    video = cv2.VideoCapture(file_name)\n",
    "    total_frame = video.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    mtcnn_detector = mtcnn.mtcnn.MTCNN()\n",
    "    #print(file_name + '  ' + str(total_frame))\n",
    "    divid_no = 1\n",
    "    \n",
    "    if total_frame > frame_max:\n",
    "        total_frame_int = int(total_frame)\n",
    "        if total_frame_int % frame_max == 0:\n",
    "            divid_no = int(total_frame / frame_max)\n",
    "        else:\n",
    "            divid_no = int(total_frame / frame_max) + 1\n",
    "        \n",
    "    batch_no = 0\n",
    "    batch_frames = []\n",
    "    batch_frames_flip = []\n",
    "    counter = 0\n",
    "    if 'Normal' in file_name:\n",
    "        print(\"\\n\\nNORMAL\\n\\n\")\n",
    "        if divid_no != 1:\n",
    "            slice_no = int(random.random()*divid_no)\n",
    "            passby = 0\n",
    "            if slice_no != divid_no - 1:\n",
    "                while video.isOpened and passby < frame_max * slice_no:\n",
    "                    passby += 1\n",
    "                    success, image = video.read()\n",
    "                    if success == False:\n",
    "                        break\n",
    "            else:\n",
    "                while video.isOpened and passby < total_frame - frame_max:\n",
    "                    passby += 1\n",
    "                    success, image = video.read()\n",
    "                    if success == False:\n",
    "                        break\n",
    "\n",
    "    while video.isOpened:               \n",
    "        success, image = video.read()\n",
    "        if success == False:\n",
    "            break\n",
    "            \n",
    "        #ratio = image.shape[0] / image.shape[1]\n",
    "        #print(str(image.shape[0])+ ' ' + str(image.shape[1]))\n",
    "        #image = cv2.resize(image, (800, int(800*ratio)))\n",
    "        #print(image.shape)\n",
    "        #faces = face_detector.detectMultiScale(image,1.1,8)\n",
    "        '''\n",
    "        faces = mtcnn_detector.detect_faces(image)\n",
    "        \n",
    "        for face in faces:\n",
    "            (x,y,w,h) = face['box']\n",
    "            #print(face)\n",
    "            cv2.rectangle(image,(x,y), (x+w,y+h), (255,255,0), 2)\n",
    "            cv2.putText(image, str(face['confidence'])[:4], (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (36,255,12), 1)\n",
    "        '''\n",
    "        image = cv2.resize(image, (target_width, target_height))\n",
    "        image_flip = cv2.flip(image, 1)\n",
    "        \n",
    "        image_array = np.array(image)/255.0\n",
    "        image_array_flip = np.array(image_flip)/255.0\n",
    "        \n",
    "        batch_frames.append(image_array)\n",
    "        batch_frames_flip.append(image_array_flip)\n",
    "        \n",
    "        counter += 1\n",
    "        if counter > frame_max:\n",
    "            break\n",
    "            \n",
    "    video.release()\n",
    "    batch_frames = np.array(batch_frames)\n",
    "    #print(batch_frames.shape)\n",
    "        \n",
    "    return np.expand_dims(batch_frames,0), np.expand_dims(batch_frames_flip, 0), total_frame\n",
    "\n",
    "def generate_input():\n",
    "    #has_visited = [0 for i in range(len(train_fn))]\n",
    "    \n",
    "    print(\"\\n\\nGENERATE_INPUT\\n\")\n",
    "    loop_no = 0\n",
    "    while 1:\n",
    "        index = update_index[loop_no]\n",
    "        loop_no += 1\n",
    "        if loop_no == len(train_fn):\n",
    "            loop_no = 0\n",
    "            \n",
    "        #index = 0\n",
    "        batch_frames, batch_frames_flip, total_frames = input_video_data(train_fn[index])\n",
    "        print(\"\\ntrain_fn[\",index,\"]=\",train_fn[index],\"\\ntotal_frames=\",total_frames,\"\\nbatch_frames.shape=\",batch_frames.shape)\n",
    "        #if batch_frames.ndim != 5:\n",
    "        #   break\n",
    "        \n",
    "        # GENERATORS    \n",
    "        #       a kind of iterators that can only iterate over once\n",
    "        #       NO store of values in memory\n",
    "        # YIELD \n",
    "        #       like a return, except the function will return a generator\n",
    "        \n",
    "        '''\n",
    "        if 'Abuse' in train_fn[index]:\n",
    "            yield batch_frames,  np.array([np.array([1,0,0,0,0,0,0,0,0,0,0,0,0,0])])\n",
    "        elif 'Arrest' in train_fn[index]:\n",
    "            yield batch_frames,  np.array([np.array([0,1,0,0,0,0,0,0,0,0,0,0,0,0])])\n",
    "        elif 'Arson' in train_fn[index]:\n",
    "            yield batch_frames,  np.array([np.array([0,0,1,0,0,0,0,0,0,0,0,0,0,0])])\n",
    "        elif 'Assault' in train_fn[index]:\n",
    "            yield batch_frames,  np.array([np.array([0,0,0,1,0,0,0,0,0,0,0,0,0,0])]) \n",
    "        elif 'Burglary' in train_fn[index]:\n",
    "            yield batch_frames,  np.array([np.array([0,0,0,0,1,0,0,0,0,0,0,0,0,0])]) \n",
    "        elif 'Explosion' in train_fn[index]:\n",
    "            yield batch_frames,  np.array([np.array([0,0,0,0,0,1,0,0,0,0,0,0,0,0])])        \n",
    "        elif 'Fighting' in train_fn[index]:\n",
    "            yield batch_frames,  np.array([np.array([0,0,0,0,0,0,1,0,0,0,0,0,0,0])]) \n",
    "        elif 'RoadAccidents' in train_fn[index]:\n",
    "            yield batch_frames,  np.array([np.array([0,0,0,0,0,0,0,1,0,0,0,0,0,0])]) \n",
    "        elif 'Robbery' in train_fn[index]:\n",
    "            yield batch_frames,  np.array([np.array([0,0,0,0,0,0,0,0,1,0,0,0,0,0])]) \n",
    "        elif 'Shooting' in train_fn[index]:\n",
    "            yield batch_frames,  np.array([np.array([0,0,0,0,0,0,0,0,0,1,0,0,0,0])]) \n",
    "        elif 'Shoplifting' in train_fn[index]:\n",
    "            yield batch_frames,  np.array([np.array([0,0,0,0,0,0,0,0,0,0,1,0,0,0])]) \n",
    "        elif 'Stealing' in train_fn[index]:\n",
    "            yield batch_frames,  np.array([np.array([0,0,0,0,0,0,0,0,0,0,0,1,0,0])])\n",
    "        elif 'Normal' in train_fn[index]:\n",
    "            yield batch_frames,  np.array([np.array([0,0,0,0,0,0,0,0,0,0,0,0,1,0])]) \n",
    "        elif 'Vandalism' in train_fn[index]:\n",
    "            yield batch_frames,  np.array([np.array([0,0,0,0,0,0,0,0,0,0,0,0,0,1])]) \n",
    "        '''\n",
    "        \n",
    "        #batch_frames\n",
    "        if 'label_A' in train_fn[index]:\n",
    "            yield batch_frames, np.array([0])   #normal\n",
    "        else:\n",
    "            yield batch_frames, np.array([1])   #abnormal\n",
    "\n",
    "        #batch_frames_flip\n",
    "        if 'label_A' in train_fn[index]:\n",
    "            yield batch_frames_flip, np.array([0])  #normal\n",
    "        else:\n",
    "            yield batch_frames_flip, np.array([1])  #abnormal\n",
    "    \n",
    "    print(\"loop_no=\",loop_no)\n",
    "\n",
    "def input_test_video_data(file_name, batch_no=0):\n",
    "    #file_name = 'C:\\\\Bosch\\\\Anomaly\\\\training\\\\videos\\\\13_007.avi'\n",
    "    #file_name = '/raid/DATASETS/anomaly/UCF_Crimes/Videos/Training_Normal_Videos_Anomaly/Normal_Videos308_x264.mp4'\n",
    "    video = cv2.VideoCapture(file_name)\n",
    "    total_frame = video.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    fps = video.get(cv2.CAP_PROP_FPS)\n",
    "    #mtcnn_detector = mtcnn.mtcnn.MTCNN()\n",
    "    divid_no = 1\n",
    "\n",
    "    if total_frame > frame_max:\n",
    "        total_frame_int = int(total_frame)\n",
    "        if total_frame_int % frame_max == 0:\n",
    "            divid_no = int(total_frame / frame_max)\n",
    "        else:\n",
    "            divid_no = int(total_frame / frame_max) + 1\n",
    "\n",
    "\n",
    "    #updates the start frame to 0,4000,8000... excluding the last batch\n",
    "    passby = 0\n",
    "    if batch_no != divid_no - 1:\n",
    "        while video.isOpened and passby < frame_max * batch_no:\n",
    "            passby += 1\n",
    "            success, image = video.read()\n",
    "            if success == False:\n",
    "                break\n",
    "            \n",
    "    #updates the last batch starting frame \n",
    "    else:\n",
    "        if batch_type==1:\n",
    "            #print(\"1\")\n",
    "            while video.isOpened and passby < total_frame - frame_max:\n",
    "                passby += 1\n",
    "                success, image = video.read()\n",
    "                if success == False:\n",
    "                    break\n",
    "        #last batch must have >= 400 otherwise it falls back to batch_type 1\n",
    "        if batch_type==2 and total_frame - (frame_max * batch_no) >= frame_max*0.1:\n",
    "            #print(\"2\")\n",
    "            while video.isOpened and passby < frame_max * batch_no:\n",
    "                passby += 1\n",
    "                success, image = video.read()\n",
    "                if success == False:\n",
    "                    break\n",
    "        else:\n",
    "            while video.isOpened and passby < total_frame - frame_max:\n",
    "                passby += 1\n",
    "                success, image = video.read()\n",
    "                if success == False:\n",
    "                    break\n",
    "\n",
    "            \n",
    "    batch_frames, batch_imgs = [], []\n",
    "    counter = 0\n",
    "    \n",
    "    while video.isOpened:               \n",
    "        success, image = video.read()\n",
    "        if success == False:\n",
    "            break\n",
    "        batch_imgs.append(image)\n",
    "        \n",
    "        image_rsz = cv2.resize(image, (target_width, target_height))\n",
    "        image_array = np.array(image_rsz)/255.0 #normalize\n",
    "        batch_frames.append(image_array)\n",
    "        \n",
    "        counter += 1\n",
    "        if counter > frame_max:\n",
    "            break\n",
    "            \n",
    "    video.release()\n",
    "    batch_frames = np.array(batch_frames)\n",
    "    \n",
    "    print(\"\\t-batch\",batch_no,\"[\",passby,\", ... ] \",batch_frames.shape)    \n",
    "    \n",
    "    return np.expand_dims(batch_frames,0), batch_imgs, divid_no, total_frame, passby, fps\n",
    "\n",
    "def watch_test(predict_total,test_files):\n",
    "    global is_quit, is_paused, frame_counter\n",
    "    for k in range(len(test_files)):\n",
    "        \n",
    "        file_path = test_files[k]\n",
    "        video = cv2.VideoCapture(str(file_path))\n",
    "        \n",
    "        window_name = \"anoml vwr\"+str(k)+\":\"+file_path.replace('/media/jtstudents/HDD/.zuble/xdviol/test','')\n",
    "        cv2.namedWindow(window_name)\n",
    "        \n",
    "        # Video information\n",
    "        fps = video.get(cv2.CAP_PROP_FPS)\n",
    "        width  = video.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "        height = video.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "        total_frame = video.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        \n",
    "        # We can set up keys to pause, go back and forth.\n",
    "        # **params can be used to pass parameters to key actions.\n",
    "        def quit_key_action(**params):\n",
    "            global is_quit\n",
    "            is_quit = True\n",
    "        def rewind_key_action(**params):\n",
    "            global frame_counter\n",
    "            frame_counter = max(0, int(frame_counter - (fps * 5)))\n",
    "            video.set(cv2.CAP_PROP_POS_FRAMES, frame_counter)\n",
    "        def forward_key_action(**params):\n",
    "            global frame_counter\n",
    "            frame_counter = min(int(frame_counter + (fps * 5)), total_frame - 1)\n",
    "            video.set(cv2.CAP_PROP_POS_FRAMES, frame_counter)\n",
    "        def pause_key_action(**params):\n",
    "            global is_paused\n",
    "            is_paused = not is_paused\n",
    "        # Map keys to buttons\n",
    "        key_action_dict = {\n",
    "            ord('q'): quit_key_action,\n",
    "            ord('a'): rewind_key_action,\n",
    "            ord('d'): forward_key_action,\n",
    "            ord('s'): pause_key_action,\n",
    "            ord(' '): pause_key_action\n",
    "        }\n",
    "        def key_action(_key):\n",
    "            if _key in key_action_dict:\n",
    "                key_action_dict[_key]()\n",
    "                \n",
    "        prev_time = time.time() # Used to track real fps\n",
    "        is_quit = False         # Used to signal that quit is called\n",
    "        is_paused = False       # Used to signal that pause is called\n",
    "        \n",
    "        frame_counter = 0       # Used to track which frame are we.\n",
    "        batch_in_video = predict_total[k][0]\n",
    "        predict_atual = ()\n",
    "        try:\n",
    "            while video.isOpened():\n",
    "                # If the video is paused, we don't continue reading frames.\n",
    "                if is_quit:\n",
    "                    # Do something when quiting\n",
    "                    break\n",
    "                elif is_paused:\n",
    "                    # Do something when paused\n",
    "                    pass\n",
    "                else:\n",
    "                    sucess, frame = video.read() # Read the frames\n",
    "                    if not sucess:break\n",
    "\n",
    "                    frame_counter = int(video.get(cv2.CAP_PROP_POS_FRAMES))\n",
    "                                        \n",
    "                    if batch_in_video == 1: predict_atual = predict_total[k][2]\n",
    "                    #(2, 4001, 0.99958295, 661, 4661, 0.98756117))\n",
    "                    if batch_in_video == 2:\n",
    "                        #<661 as=0.999\n",
    "                        if frame_counter < predict_total[k][3]: predict_atual = predict_total[k][2]\n",
    "                        # > 661 and < 4000 as: 0.999 | 0.987\n",
    "                        elif frame_counter < predict_total[k][1]: predict_atual = predict_total[k][2] , predict_total[k][5]\n",
    "                        # > 4000 as: 0.987\n",
    "                        else: predict_atual = predict_total[k][5]\n",
    "                    #(3, 4001, 0.99958295, 4001, 8000, 0.98756117,4500,8500,0.836))\n",
    "                    if batch_in_video == 3:\n",
    "                        #<4000 as=0.999\n",
    "                        if frame_counter < predict_total[k][1]: predict_atual = predict_total[k][2]\n",
    "                        #< 4500 as=0.987\n",
    "                        elif frame_counter < predict_total[k][6]: predict_atual = predict_total[k][5]\n",
    "                        #< 8000 as=0.987 | 0.836\n",
    "                        elif frame_counter < predict_total[k][4]: predict_atual = predict_total[k][5] , predict_total[k][8]\n",
    "                        # > 8000 as=0.836\n",
    "                        else:predict_atual = predict_total[k][8]\n",
    "                    #(4, 4001,0.99958295, 4001,8000,0.98756117, 8000,12000,0.836, 8500,12500,0.888))    \n",
    "                    if batch_in_video == 4:\n",
    "                        #<4000 as=0.999\n",
    "                        if frame_counter < predict_total[k][1]: predict_atual = predict_total[k][2]\n",
    "                        #<8000 as=0.987\n",
    "                        elif frame_counter < predict_total[k][4]: predict_atual = predict_total[k][5]\n",
    "                        #< 8500 as=0.836\n",
    "                        elif frame_counter < predict_total[k][9]: predict_atual = predict_total[k][8]\n",
    "                        #< 12000 as=0.836 / 0.888\n",
    "                        elif frame_counter < predict_total[k][7]: predict_atual = predict_total[k][8] , predict_total[k][11]\n",
    "                        # > 8000 as=0.836\n",
    "                        else:predict_atual = predict_total[k][11]\n",
    "                    #(5, 4001,0.99958295, 4001,8000,0.98756117, 8000,12000,0.836, 12000,16000,0.888, 12500,16500,0.777))\n",
    "                    if batch_in_video == 5:\n",
    "                        #<4000 as=0.999\n",
    "                        if frame_counter < predict_total[k][1]: predict_atual = predict_total[k][2]\n",
    "                        #<8000 as=0.987\n",
    "                        elif frame_counter < predict_total[k][4]: predict_atual = predict_total[k][5]\n",
    "                        #< 12000 as=0.836\n",
    "                        elif frame_counter < predict_total[k][7]: predict_atual = predict_total[k][8]\n",
    "                        #< 12500 as=0.888\n",
    "                        elif frame_counter < predict_total[k][12]: predict_atual = predict_total[k][11]\n",
    "                        #<16000 as:0.888 | 0.777\n",
    "                        elif frame_counter < predict_total[k][10]: predict_atual = predict_total[k][11] , predict_total[k][14]\n",
    "                        # >16000 as=0.777\n",
    "                        else:predict_atual = predict_total[k][14]\n",
    "                    #(6, 4001,0.99958295, 4001,8000,0.98756117, 8000,12000,0.836, 12000,16000,0.888, 16000,20000,0.777, 16500,20500,0.5))\n",
    "                    if batch_in_video == 6:\n",
    "                        #<4000 as=0.999\n",
    "                        if frame_counter < predict_total[k][1]: predict_atual = predict_total[k][2]\n",
    "                        #<8000 as=0.987\n",
    "                        elif frame_counter < predict_total[k][4]: predict_atual = predict_total[k][5]\n",
    "                        #< 12000 as=0.836\n",
    "                        elif frame_counter < predict_total[k][7]: predict_atual = predict_total[k][8]\n",
    "                        #<16000 as=0.888\n",
    "                        elif frame_counter < predict_total[k][10]: predict_atual = predict_total[k][11]\n",
    "                        #<16500 as:0.777\n",
    "                        elif frame_counter < predict_total[k][15]: predict_atual = predict_total[k][14]\n",
    "                        #<20000 as:0.777 | 0.5\n",
    "                        elif frame_counter < predict_total[k][13]: predict_atual = predict_total[k][14] , predict_total[k][17]\n",
    "                        #>20000 as=0.5\n",
    "                        else:predict_atual = predict_total[k][17]\n",
    "                    #(7, 4001,0.99958295, 4001,8000,0.98756117, 8000,12000,0.836, 12000,16000,0.888, 16000,20000,0.777, 20000,24000,0.5, 20500,24500,0.6))\n",
    "                    if batch_in_video == 7:\n",
    "                        #<4000 as=0.999\n",
    "                        if frame_counter < predict_total[k][1]: predict_atual = predict_total[k][2]\n",
    "                        #<8000 as=0.987\n",
    "                        elif frame_counter < predict_total[k][4]: predict_atual = predict_total[k][5]\n",
    "                        #< 12000 as=0.836\n",
    "                        elif frame_counter < predict_total[k][7]: predict_atual = predict_total[k][8]\n",
    "                        #<16000 as=0.888\n",
    "                        elif frame_counter < predict_total[k][10]: predict_atual = predict_total[k][11]\n",
    "                        #<20000 as=0.777\n",
    "                        elif frame_counter < predict_total[k][13]: predict_atual = predict_total[k][14]\n",
    "                        #<20500 as:0.5\n",
    "                        elif frame_counter < predict_total[k][18]: predict_atual = predict_total[k][17]\n",
    "                        #<24000 as:0.5 | 0.6\n",
    "                        elif frame_counter < predict_total[k][16]: predict_atual = predict_total[k][17] , predict_total[k][20]\n",
    "                        #>24000 as=0.6\n",
    "                        else:predict_atual = predict_total[k][20]\n",
    "                    #(8, 4001,0.2, 4001,8000,0.5, 8000,12000,0.8, 12000,16000,0.11, 16000,20000,0.14, 20000,24000,0.17, 24000,28000,0.20, 24500.28500,0.23))\n",
    "                    if batch_in_video == 8:\n",
    "                        #<4000 as=0.2\n",
    "                        if frame_counter < predict_total[k][1]: predict_atual = predict_total[k][2]\n",
    "                        #<8000 as=0.5\n",
    "                        elif frame_counter < predict_total[k][4]: predict_atual = predict_total[k][5]\n",
    "                        #< 12000 as=0.8\n",
    "                        elif frame_counter < predict_total[k][7]: predict_atual = predict_total[k][8]\n",
    "                        #<16000 as=0.11\n",
    "                        elif frame_counter < predict_total[k][10]: predict_atual = predict_total[k][11]\n",
    "                        #<20000 as=0.14\n",
    "                        elif frame_counter < predict_total[k][13]: predict_atual = predict_total[k][14]\n",
    "                        #<24000 as:0.17\n",
    "                        elif frame_counter < predict_total[k][16]: predict_atual = predict_total[k][17]\n",
    "                        #<24500 as:0.20\n",
    "                        elif frame_counter < predict_total[k][21]: predict_atual = predict_total[k][20]\n",
    "                        #<28000 as:0.20 | 0.23\n",
    "                        elif frame_counter < predict_total[k][19]: predict_atual = predict_total[k][20] , predict_total[k][23]\n",
    "                        #>28000 as=0.23\n",
    "                        else:predict_atual = predict_total[k][23]                    \n",
    "                    \n",
    "                    \n",
    "                    '''\n",
    "                    for current frame, check all visible items\n",
    "                    if str(frame_counter) in data_frames:\n",
    "                        for item in data_frames[str(frame_counter)]:\n",
    "                            item_id = item['visibleObjectId']\n",
    "                            color = colormap.get_color(item_id)\n",
    "                        \n",
    "                            # for visible item, get position at current frame and paint rectangle in\n",
    "                            if frame_counter in data_objects[item_id]:\n",
    "                                bbox = data_objects[item_id][frame_counter]['rectangle']\n",
    "                                x1 = bbox[0]['x']\n",
    "                                y1 = bbox[0]['y']\n",
    "                                x2 = x1 + bbox[0]['w']\n",
    "                                y2 = y1 + bbox[0]['h']\n",
    "                                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "                                cv2.putText(frame, str(item_id[:3]), (x1, y1-10), font, 0.5, color, 2)\n",
    "                    '''\n",
    "                    \n",
    "                    print(predict_atual)\n",
    "                    \n",
    "                    # Display frame numba/AS/time/fps\n",
    "                    cv2.putText(frame, 'Frame: %d' % (frame_counter), (10, 10), font, 0.5, [60,250,250], 2)\n",
    "                    cv2.putText(frame, 'AS:'+str(predict_atual), (10, 30), font, 0.5, [80,100,250], 2)\n",
    "                    \n",
    "                    cv2.putText(frame, 'Time: %.4f' % (frame_counter/fps), (int(width*2/8), 10), font, 0.5, [100,250,10], 2)\n",
    "                    new_time = time.time()\n",
    "                    cv2.putText(frame, 'fps: %.2f' % (1/(new_time-prev_time)), (int(width*4/8), 10), font, 0.5, [0,50,200], 2)\n",
    "                    prev_time = new_time\n",
    "                    \n",
    "                # Display the image\n",
    "                cv2.imshow(window_name,frame)\n",
    "\n",
    "                # Wait for any key press and pass it to the key action\n",
    "                frame_time_ms = int(1000/fps)\n",
    "                key = cv2.waitKey(frame_time_ms)\n",
    "                key_action(key)\n",
    "                \n",
    "        finally:\n",
    "            video.release()\n",
    "            cv2.destroyAllWindows()\n",
    "\n",
    "    file_number =+ 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" MODEL \"\"\"\n",
    "\n",
    "def all_operations(args):\n",
    "    x = args[0]\n",
    "    #tf.print(x.shape)\n",
    "    x = tf.reshape(x, [1, -1,x.shape[1]*x.shape[2]*x.shape[3]])\n",
    "    return x\n",
    "@tf.function\n",
    "def loss_category(y_true, y_pred):    \n",
    "    #tf.print(y_pred, y_true, 'Prediction')\n",
    "    cce = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "    return cce\n",
    "\n",
    "def gelu(x):\n",
    "    return 0.5*x*(1+tf.tanh(np.sqrt(2/np.pi)*(x+0.044715*tf.pow(x, 3))))\n",
    "#get_custom_objects().update({'gelu': Activation(gelu)})\n",
    "\n",
    "def find_weights(path,find_string): \n",
    "    weights_fn = []\n",
    "    weights_path = []\n",
    "\n",
    "    for file in os.listdir(path):\n",
    "        fname, fext = os.path.splitext(file)\n",
    "        if fext == \".h5\" and file.find(find_string) != -1 :\n",
    "            print(file)\n",
    "            weights_path.append(os.path.join(path, file))\n",
    "            weights_fn.append(file)\n",
    "\n",
    "    return weights_fn, weights_path\n",
    "\n",
    "\n",
    "def form_model(ativa,optima):\n",
    "    print(\"\\nFORM_MODEL\\n\")\n",
    "    image_input = keras.Input(shape=(None, target_height, target_width, 3))\n",
    "    #Freeze the batch normalization\n",
    "    \n",
    "    c3d_layer1 = keras.layers.Conv3D(4,(2,3,3), activation=ativa)(image_input)\n",
    "    #c3d_layer1 = keras.layers.Activation(activation=ativa)(c3d_layer1) #another way\n",
    "    c3d_pooling1 = keras.layers.MaxPooling3D((1,2,2))(c3d_layer1)\n",
    "    c3d_layer2 = keras.layers.Conv3D(8,(4,3,3), activation=ativa)(c3d_pooling1)\n",
    "    c3d_pooling2 = keras.layers.MaxPooling3D((2,2,2))(c3d_layer2)\n",
    "    c3d_layer3 = keras.layers.Conv3D(16,(8,3,3), activation=ativa)(c3d_pooling2)\n",
    "    c3d_pooling3 = keras.layers.MaxPooling3D((4,2,2))(c3d_layer3)\n",
    "    #c3d_layer4 = keras.layers.Conv3D(32,(2,3,3), activation=activa)(c3d_pooling3)\n",
    "    #c3d_pooling4 = keras.layers.MaxPooling3D((2,2,2))(c3d_layer4)\n",
    "    \n",
    "    feature_conv_4 = keras.layers.Lambda(all_operations)(c3d_pooling3) #flatten spatial features to time series\n",
    "    \n",
    "    lstm1 = keras.layers.LSTM(1024,input_shape=(1200,feature_conv_4.shape[2]), return_sequences=True)(feature_conv_4)\n",
    "    #lstm2 = keras.layers.LSTM(512, return_sequences=True)(lstm1)\n",
    "    global_feature = keras.layers.GlobalMaxPooling1D()(lstm1)\n",
    "    \n",
    "    #ADD THE AUDIO FEATURE HERE \n",
    "    \n",
    "    dense_1 = keras.layers.Dense(128, activation=ativa)(global_feature)\n",
    "    \n",
    "    #dense_2 = keras.layers.Dense(13, activation='sigmoid')(dense_1)\n",
    "    soft_max = keras.layers.Dense(1, activation='sigmoid')(dense_1)\n",
    "    \n",
    "    model = keras.Model(inputs=[image_input], outputs=[soft_max])\n",
    "    model.summary()\n",
    "    \n",
    "\n",
    "    #class_weights = [10,10,10,10,10,10,10,10,10,10,10,10,0.1,10]\n",
    "    sgd = keras.optimizers.SGD(learning_rate = 0.0002)\n",
    "    adam = keras.optimizers.Adam(learning_rate = 0.0002)\n",
    "    METRICS = [\n",
    "        keras.metrics.TruePositives(name='tp'),\n",
    "        keras.metrics.FalsePositives(name='fp'),\n",
    "        keras.metrics.TrueNegatives(name='tn'),\n",
    "        keras.metrics.FalseNegatives(name='fn'), \n",
    "        keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "        keras.metrics.Precision(name='precision'),\n",
    "        keras.metrics.Recall(name='recall'),\n",
    "        keras.metrics.AUC(name='auc'),\n",
    "        keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "    ]\n",
    "\n",
    "    model.compile(optimizer=optima, \n",
    "                    loss= 'binary_crossentropy', \n",
    "                    #loss_weights = class_weights,\n",
    "                    #metrics=['accuracy']\n",
    "                    metrics=METRICS)\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(model,model_name,weights_path):\n",
    "    '''\n",
    "    MODEL TRAIN/VALIDATION \n",
    "    (silent mode - verbose = 0)\n",
    "    '''\n",
    "    \n",
    "    if not os.path.exists(ckpt_path+'/'+time_str+model_name):\n",
    "        os.makedirs(ckpt_path+'/'+time_str+model_name)\n",
    "    #https://keras.io/api/callbacks/model_checkpoint/\n",
    "    checkpoint = ModelCheckpoint(filepath=ckpt_path+'/'+time_str+model_name+'/'+time_str+model_name+'_ckpt-{epoch:08d}.h5')\n",
    "\n",
    "    #para_file_name = '.262731_2_4_8_xdviolence_anomaly_00000010.h5'\n",
    "    #model.load_weights(para_file_name)\n",
    "\n",
    "    print(\"\\n\\nMODEL.FIT\")\n",
    "    history = model.fit(generate_input(), \n",
    "                        steps_per_epoch=len(train_fn)*2, \n",
    "                        epochs=30, \n",
    "                        verbose=1, \n",
    "                        callbacks=[checkpoint, TqdmCallback(verbose=2)])\n",
    "\n",
    "    model.save(model_path + time_str + model_name + '.h5')\n",
    "    model.save(model_path + time_str + model_name )\n",
    "    model.save_weights(weights_path + time_str + model_name + '_weights.h5')  \n",
    "\n",
    "    hist_df = pd.DataFrame(history.history)\n",
    "    hist_csv_file = hist_path + time_str + model_name + '_history.csv'\n",
    "    with open(hist_csv_file, mode = 'w') as f:\n",
    "        hist_df.to_csv(f)\n",
    "        \n",
    "    return model\n",
    "\n",
    "\n",
    "def test_model(model, files, rslt_path, model_weight_fn=''):\n",
    "    print(\"\\nTEST MODEL\\n\")\n",
    "    txt_fn = rslt_path+model_weight_fn+'_'+str(batch_type)+'.txt'\n",
    "    f = open(txt_fn, 'w')\n",
    "    content_str = ''\n",
    "    total_frames_test = 0\n",
    "    \n",
    "    predict_total = [] #to output predict in vizualizer accordingly to the each batch prediction\n",
    "    predict_max = 0 #to print the max predict related to the file in test\n",
    "    predict_total_max = [] #to perform the metrics\n",
    "    \n",
    "    start_test = time.time()\n",
    "    for i in range(len(files)):\n",
    "        if files[i] != '':\n",
    "            file_path = files[i]\n",
    "            predict_result = () #to save predictions per file\n",
    "            \n",
    "            #the frist 4000 frames from actual test video                \n",
    "            batch_frames, batch_imgs, divid_no, total_frames,start_frame, fps = input_test_video_data(file_path)\n",
    "            video_time = total_frames/fps\n",
    "            total_frames_test += total_frames\n",
    "            \n",
    "            #prediction on frist batch\n",
    "            start_predict1 = time.time()\n",
    "            predict_aux = model.predict(batch_frames)[0][0]\n",
    "            end_predict1 = time.time()\n",
    "            time_predict = end_predict1-start_predict1\n",
    "            \n",
    "            predict_max = predict_aux\n",
    "            predict_result = (divid_no,start_frame+batch_frames.shape[1],predict_max)\n",
    "            #print(predict_result,batch_frames.shape)\n",
    "            \n",
    "            high_score_patch = 0\n",
    "            print(\"\\t \",predict_max,\"%\") \n",
    "            \n",
    "            #when batch_frames (input video) has > 4000 frames\n",
    "            patch_num = 1\n",
    "            while patch_num < divid_no:\n",
    "                batch_frames, batch_imgs, divid_no, total_frames,start_frame, fps = input_test_video_data(file_path, patch_num)\n",
    "                \n",
    "                #nésimo batch prediction\n",
    "                start_predict2 = time.time()\n",
    "                predict_aux = model.predict(batch_frames)[0][0]\n",
    "                end_predict2 = time.time()\n",
    "                time_predict += end_predict2 - start_predict2\n",
    "\n",
    "                if predict_aux > predict_max:\n",
    "                    predict_max = predict_aux\n",
    "                    high_score_patch = patch_num\n",
    "                \n",
    "                predict_result += (start_frame,start_frame+batch_frames.shape[1], predict_aux)\n",
    "                #print(predict_result)\n",
    "                \n",
    "                print(\"\\t \",predict_aux,\"%\")  \n",
    "                patch_num += 1\n",
    "            \n",
    "            predict_total.append(predict_result)\n",
    "            predict_total_max.append(predict_max)\n",
    "            print(predict_total[i])\n",
    "            \n",
    "            if 'label_A' in files[i]:\n",
    "                print('\\nNORM',str(i),':',f'{total_frames:.0f}',\"@\",f'{fps:.0f}',\"fps =\",f'{video_time:.2f}',\"sec\\n\\t\",\n",
    "                        files[i][files[i].rindex('/')+1:],\n",
    "                        \"\\n\\t \"+str(predict_max),\"% @batch\",high_score_patch,\"in\",str(time_predict),\"seconds\\n\",\n",
    "                        \"----------------------------------------------------\\n\")\n",
    "            else:\n",
    "                print('\\nABNORM',str(i),':',f'{total_frames:.0f}',\"@\",f'{fps:.0f}',\"fps =\",f'{video_time:.2f}',\"sec\\n\\t\",\n",
    "                        files[i][files[i].rindex('/')+1:],\n",
    "                        \"\\n\\t\"+str(predict_max),\"% @batch\",high_score_patch,\"in\",str(time_predict),\"seconds\\n\",\n",
    "                        \"----------------------------------------------------\\n\")\n",
    "                \n",
    "            content_str += files[i][files[i].rindex('/')+1:] + '|' + str(predict_total_max[i]) + '|' + str(predict_total[i])  + '\\n'\n",
    "            \n",
    "    end_test = time.time()\n",
    "    time_test = end_test - start_test\n",
    "\n",
    "    f.write(content_str)\n",
    "    f.close()\n",
    "    print(\"\\nDONE\\n\\ttotal of\",str(total_frames_test),\"frames processed in\",time_test,\" seconds\",\n",
    "            \"\\n\\t\"+str(total_frames_test / time_test),\"frames per second\",\n",
    "            \"\\n\\n********************************************************\",\n",
    "            \"\\n\\n********************************************************\")                  \n",
    "\n",
    "    #remove white spaces in file , for further easier reading\n",
    "    with open(txt_fn, 'r+') as f:\n",
    "        txt = f.read().replace(' ', '')\n",
    "        f.seek(0)\n",
    "        f.write(txt)\n",
    "        f.truncate()\n",
    "    \n",
    "    return predict_total_max, predict_total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" METRICS\n",
    "    https://www.tensorflow.org/tutorials/structured_data/imbalanced_data \"\"\"\n",
    "\n",
    "def plot_cm(name,labels,predictions,p=0.5):\n",
    "    '''\n",
    "    https://www.tensorflow.org/tutorials/structured_data/imbalanced_data#download_the_kaggle_credit_card_fraud_data_set\n",
    "    '''\n",
    "    predictions = np.array(predictions)\n",
    "    cm = confusion_matrix(labels, predictions > p)\n",
    "    #plt.clf()\n",
    "    plt.figure(figsize=(5,5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "    plt.title('Confusion matrix @{:.2f}'.format(p))\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.savefig(name+'.png',facecolor='white', transparent=False)\n",
    "    plt.show()\n",
    "\n",
    "def plot_roc(name, labels, predictions, **kwargs):\n",
    "    fp, tp, _ = sklearn.metrics.roc_curve(labels, predictions)\n",
    "    \n",
    "    plt.plot(100*fp, 100*tp, label=name, linewidth=2, **kwargs)\n",
    "    plt.xlabel('False positives [%]')\n",
    "    plt.ylabel('True positives [%]')\n",
    "    plt.xlim([-0.5,80])\n",
    "    plt.ylim([20,100.5])\n",
    "    plt.grid(True)\n",
    "    ax = plt.gca()\n",
    "    ax.set_aspect('equal')\n",
    "    plt.savefig(name+'.png',facecolor='white', transparent=False)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_prc(name, labels, predictions, **kwargs):\n",
    "    precision, recall, _ = sklearn.metrics.precision_recall_curve(labels, predictions)\n",
    "    #plt.clf()\n",
    "    plt.plot(precision, recall, label=name, linewidth=2, **kwargs)\n",
    "    plt.xlabel('Precision')\n",
    "    plt.ylabel('Recall')\n",
    "    plt.xlim([-0.5,100.5])\n",
    "    plt.ylim([-0.5,100.5])\n",
    "    plt.grid(True)\n",
    "    ax = plt.gca()\n",
    "    ax.set_aspect('equal')\n",
    "    plt.savefig(name+'.png',facecolor='white', transparent=False)\n",
    "    plt.show()\n",
    "\n",
    "def get_precision_recall_f1(labels, predictions):\n",
    "    p = tf.keras.metrics.Precision(thresholds = 0.5)\n",
    "    p.update_state(labels, predictions)\n",
    "    p_res = p.result().numpy()\n",
    "    print(\"\\tPRECISION (%% of True Positive out of all Positive predicted) \",p_res)\n",
    "    \n",
    "    r = tf.keras.metrics.Recall(thresholds=0.5)\n",
    "    r.update_state(labels, predictions)\n",
    "    r_res = r.result().numpy()\n",
    "    print(\"\\tRECALL (%% of True Positive out of all actual anomalies) \",r_res)\n",
    "    \n",
    "    #https://glassboxmedicine.com/2019/03/02/measuring-performance-auprc/\n",
    "    auprc_ap = sklearn.metrics.average_precision_score(labels, predictions)\n",
    "    print(\"\\tAP ( AreaUnderPrecisionRecallCurve )\",auprc_ap)\n",
    "    \n",
    "    #https://www.tensorflow.org/addons/api_docs/python/tfa/metrics/F1Score\n",
    "    #import tensorflow_addons as tfa\n",
    "    #f1 = tfa.metrics.F1Score(num_classes=2, threshold=0.5)\n",
    "    #f1.update_state(labels, predictions)\n",
    "    f1_res = 2*((p_res*r_res)/(p_res+r_res+K.epsilon()))\n",
    "    print(\"\\tF1_SCORE (harmonic mean of precision and recall) \",f1_res)\n",
    "    \n",
    "    return p_res,r_res,auprc_ap,f1_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' GPU CONFIGURATION\n",
    "    https://www.tensorflow.org/guide/gpu '''\n",
    "\n",
    "def set_tf_loglevel(level):\n",
    "    if level >= logging.FATAL:\n",
    "        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "    if level >= logging.ERROR:\n",
    "        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "    if level >= logging.WARNING:\n",
    "        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "    else:\n",
    "        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'\n",
    "    logging.getLogger('tensorflow').setLevel(level)\n",
    "\n",
    "set_tf_loglevel(logging.WARNING)\n",
    "tf.debugging.set_log_device_placement(False) #Enabling device placement logging causes any Tensor allocations or operations to be printed.\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "#https://www.tensorflow.org/api_docs/python/tf/config/experimental/set_memory_growth\n",
    "#if gpus:\n",
    "#    print(\"\\nAvaiable GPU's\",gpus)\n",
    "#    try:\n",
    "#        # Currently, memory growth needs to be the same across GPUs\n",
    "#        for gpu in gpus:\n",
    "#            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "#        \n",
    "#        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "#        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "#    except RuntimeError as e:\n",
    "#        # Memory growth must be set before GPUs have been initialized\n",
    "#        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" ZHEN .h5 FILES \"\"\"\n",
    "\n",
    "#https://stackoverflow.com/questions/845058/how-to-get-line-count-of-a-large-file-cheaply-in-python/68385697#68385697\n",
    "def buf_count_newlines_gen(fname):\n",
    "    def _make_gen(reader):\n",
    "        while True:\n",
    "            b = reader(2 ** 16)\n",
    "            if not b: break\n",
    "            yield b\n",
    "\n",
    "    with open(fname, \"rb\") as f:\n",
    "        count = sum(buf.count(b\"\\n\") for buf in _make_gen(f.raw.read))\n",
    "    return count\n",
    "\n",
    "def get_results_from_txt(rslt_path):\n",
    "    res_txt_fn = []\n",
    "    res_model_fn = []\n",
    "    \n",
    "    for file in os.listdir(rslt_path):\n",
    "        fname, fext = os.path.splitext(file)\n",
    "        if fext == \".txt\" and file.find('xdviolence') != -1:\n",
    "            res_txt_fn.append(os.path.join(rslt_path, file))\n",
    "            res_model_fn.append(fname)\n",
    "    \n",
    "    res_txt_fn = sorted(res_txt_fn)\n",
    "    res_model_fn = sorted(res_model_fn)\n",
    "    \n",
    "\n",
    "    i=0\n",
    "    res_list_full = [[() for i in range(len(res_txt_fn))] for j in range(buf_count_newlines_gen(res_txt_fn[i]))]\n",
    "    res_list_max = [[0.0 for i in range(len(res_txt_fn))] for j in range(buf_count_newlines_gen(res_txt_fn[i]))]\n",
    "    res_list_fn = [['' for i in range(len(res_txt_fn))] for j in range(buf_count_newlines_gen(res_txt_fn[i]))]\n",
    "    print(np.shape(res_list_full))\n",
    "    \n",
    "    n_models = len(res_txt_fn)\n",
    "    for txt_i in range(n_models):\n",
    "        print('\\nOPENING',res_txt_fn[txt_i])\n",
    "        txt = open(res_txt_fn[txt_i],'r')\n",
    "        txt_data = txt.read()\n",
    "        txt.close()\n",
    "\n",
    "        video_list = [line.split() for line in txt_data.split(\"\\n\") if line]\n",
    "        #print(video_list)\n",
    "        for video_j in range(len(video_list)):\n",
    "            aux_line = str(video_list[video_j]).replace('[','').replace(']','').replace(' ','').split('|')\n",
    "        \n",
    "            res_list_fn[video_j][txt_i] = aux_line[0]\n",
    "            res_list_max[video_j][txt_i] = float(aux_line[1])\n",
    "            \n",
    "            aux2_line = aux_line[2].replace(' ','').replace(\"'\",\"\").replace('(','').replace(')','').split(',')\n",
    "            #print(aux2_line)\n",
    "            res_list_full[video_j][txt_i] = aux2_line\n",
    "    \n",
    "    \n",
    "    res_list_labels = [[0 for i in range(len(res_txt_fn))] for j in range(buf_count_newlines_gen(res_txt_fn[i]))]\n",
    "    n_videos = np.shape(res_list_fn)[0]\n",
    "    for txt_i in range(n_models):\n",
    "        for video_j in range(n_videos):\n",
    "            if 'label_A' not in res_list_fn[video_j][txt_i]:\n",
    "                res_list_labels[video_j][txt_i] = 1\n",
    "        \n",
    "    for i in range(len(res_txt_fn)):\n",
    "        print(\"\\nresults for\",res_model_fn[i])\n",
    "            \n",
    "        res_col = [col[i] for col in res_list_max]\n",
    "        labels_col = [col[i] for col in res_list_labels]\n",
    "        get_precision_recall_f1(labels_col,res_col)\n",
    "        \n",
    "    return res_list_full, res_list_max, res_list_fn, res_list_labels\n",
    "\n",
    "def test_zhen_h5():\n",
    "    model = form_model(ativa='relu',optima='sgd')\n",
    "    weights_fn, weights_path = find_weights(weights_path_zhen,'_2_4_8_xdviolence_model_weights')\n",
    "    onev_fn, y_onev_labels = test_files(onev = 10)\n",
    "    \n",
    "    for i in range(len(weights_fn)):\n",
    "        print(\"\\n\\nLoading weights from\",weights_fn[i],\"\\n\")\n",
    "        model.load_weights(weights_path[i])\n",
    "        weight_fn,weight_ext = os.path.splitext(str(weights_fn[i]))\n",
    "        \n",
    "        y_test_pred, predict_total = test_model(model, onev_fn, rslt_path_zhen, model_weight_fn = weight_fn)\n",
    "        plot_cm(rslt_path_zhen+'/1V/'+weight_fn+'_CM'+str(batch_type),y_onev_labels, y_test_pred)\n",
    "        plot_roc(rslt_path_zhen+'/1V/'+weight_fn+'_ROC'+str(batch_type), y_onev_labels , y_test_pred, color=colors[0], linestyle='--')\n",
    "        plot_prc(rslt_path_zhen+'/1V/'+weight_fn+'_PRC'+str(batch_type), y_onev_labels, y_test_pred, color=colors[0])\n",
    "        get_precision_recall_f1(y_onev_labels, y_test_pred)\n",
    "        #watch_test(predict_total,onev_fn)\n",
    "        \n",
    "    ''' \n",
    "    =1 last batch has 4000 frames \n",
    "    =2 last batch has no repetead frames \n",
    "    '''\n",
    "#batch_type = 2\n",
    "#print(\"\\n\\n\\tBATCH TYPE\",batch_type)\n",
    "\n",
    "#test_zhen_h5()\n",
    "\n",
    "#res_list_full,res_list_max,rest_list_fn,res_list_labels = get_results_from_txt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" GELU \"\"\"\n",
    "#https://keras.io/guides/distributed_training/\n",
    "#strategy = tf.distribute.MirroredStrategy()\n",
    "#print(\"Number of devices: {}\".format(strategy.num_replicas_in_sync))\n",
    "'''Everything that creates variables should be under the strategy scope.In general this is only model construction & `compile()` '''\n",
    "#with strategy.scope():\n",
    "#    model_gelu = form_model(ativa = 'gelu')\n",
    "#model_gelu = train_model(model_gelu,'_3gelu_xdviolence',weights_path_zu)\n",
    "\n",
    "\n",
    "''' load model from model_save '''\n",
    "#weights_fn, weights_path = find_weights(model_path,'_3gelu_xdviolence')\n",
    "#print(weights_fn,weights_path)\n",
    "\n",
    "##load model error with activation\n",
    "#model_gelu = keras.models.load_model(weights_path[0],custom_objects={'gelu': Activation(gelu)})\n",
    "\n",
    "\n",
    "''' create model arch and loads weights '''\n",
    "#model_gelu = form_model(ativa = tf.keras.activations.gelu)\n",
    "#model_gelu = form_model(ativa = \"gelu\",optima='sgd')\n",
    "\n",
    "#weights_fn, weights_path = find_weights(weights_path_zu,'_3gelu_xdviolence')\n",
    "#print(weights_fn,weights_path)\n",
    "\n",
    "#https://stackoverflow.com/questions/72524486/i-get-this-error-attributeerror-nonetype-object-has-no-attribute-predict\n",
    "#model_gelu.load_weights(str(weights_path[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FORM_MODEL\n",
      "\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[3744,4096] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Add]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-c8a8737f6789>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_relu_sgd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mform_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mativa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptima\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sgd'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel_relu_sgd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_relu_sgd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'_3relu_sgd_xdviolence'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweights_path_zu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel_gelu_adam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mform_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mativa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"gelu\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptima\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel_gelu_adam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_gelu_adam\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'_3gelu_adam_xdviolence'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweights_path_zu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-3dcdfaf7e381>\u001b[0m in \u001b[0;36mform_model\u001b[0;34m(ativa, optima)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mfeature_conv_4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_operations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc3d_pooling3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#flatten spatial features to time series\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mlstm1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeature_conv_4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_conv_4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0;31m#lstm2 = keras.layers.LSTM(512, return_sequences=True)(lstm1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mglobal_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGlobalMaxPooling1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/zhen_gpu/lib/python3.6/site-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 654\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m     \u001b[0;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/zhen_gpu/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    895\u001b[0m           \u001b[0;31m# Build layer if applicable (if the `build` method has been\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m           \u001b[0;31m# overridden).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 897\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    898\u001b[0m           \u001b[0mcast_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/zhen_gpu/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2414\u001b[0m         \u001b[0;31m# operations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2415\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_init_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2416\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2417\u001b[0m       \u001b[0;31m# We must set also ensure that the layer is marked as built, and the build\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2418\u001b[0m       \u001b[0;31m# shape is stored since user defined build functions may not be calling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/zhen_gpu/lib/python3.6/site-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    571\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_input_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/zhen_gpu/lib/python3.6/site-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(instance, input_shape)\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m       \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_tuples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m     \u001b[0moutput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m     \u001b[0;31m# Return shapes from `fn` as TensorShapes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moutput_shape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/zhen_gpu/lib/python3.6/site-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m   2322\u001b[0m         \u001b[0mregularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_regularizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2323\u001b[0m         \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_constraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2324\u001b[0;31m         caching_device=default_caching_device)\n\u001b[0m\u001b[1;32m   2325\u001b[0m     self.recurrent_kernel = self.add_weight(\n\u001b[1;32m   2326\u001b[0m         \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munits\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/zhen_gpu/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36madd_weight\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner, use_resource, synchronization, aggregation, **kwargs)\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m         caching_device=caching_device)\n\u001b[0m\u001b[1;32m    578\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mregularizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m       \u001b[0;31m# TODO(fchollet): in the future, this should be handled at the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/zhen_gpu/lib/python3.6/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_add_variable_with_custom_getter\u001b[0;34m(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\u001b[0m\n\u001b[1;32m    741\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 743\u001b[0;31m         **kwargs_for_getter)\n\u001b[0m\u001b[1;32m    744\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m     \u001b[0;31m# If we set an initializer and the variable processed it, tracking will not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/zhen_gpu/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer_utils.py\u001b[0m in \u001b[0;36mmake_variable\u001b[0;34m(name, shape, dtype, initializer, trainable, caching_device, validate_shape, constraint, use_resource, collections, synchronization, aggregation, partitioner)\u001b[0m\n\u001b[1;32m    139\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m       \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m       shape=variable_shape if variable_shape else None)\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/zhen_gpu/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    257\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mVariableV1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v1_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/zhen_gpu/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m_variable_v1_call\u001b[0;34m(cls, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope, constraint, use_resource, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         shape=shape)\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m   def _variable_v2_call(cls,\n",
      "\u001b[0;32m~/anaconda3/envs/zhen_gpu/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m                         shape=None):\n\u001b[1;32m    197\u001b[0m     \u001b[0;34m\"\"\"Call on Variable class. Useful to force the signature.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdefault_variable_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creator_stack\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m       \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_getter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_getter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/zhen_gpu/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mdefault_variable_creator\u001b[0;34m(next_creator, **kwargs)\u001b[0m\n\u001b[1;32m   2596\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2597\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2598\u001b[0;31m         shape=shape)\n\u001b[0m\u001b[1;32m   2599\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2600\u001b[0m     return variables.RefVariable(\n",
      "\u001b[0;32m~/anaconda3/envs/zhen_gpu/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariableMetaclass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/zhen_gpu/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m   1432\u001b[0m           \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m           \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1434\u001b[0;31m           distribute_strategy=distribute_strategy)\n\u001b[0m\u001b[1;32m   1435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1436\u001b[0m   def _init_from_args(self,\n",
      "\u001b[0;32m~/anaconda3/envs/zhen_gpu/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m_init_from_args\u001b[0;34m(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, distribute_strategy, shape)\u001b[0m\n\u001b[1;32m   1565\u001b[0m           \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Initializer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1566\u001b[0m             initial_value = ops.convert_to_tensor(\n\u001b[0;32m-> 1567\u001b[0;31m                 \u001b[0minitial_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minit_from_fn\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minitial_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1568\u001b[0m                 name=\"initial_value\", dtype=dtype)\n\u001b[1;32m   1569\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/zhen_gpu/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer_utils.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    119\u001b[0m         (type(init_ops.Initializer), type(init_ops_v2.Initializer))):\n\u001b[1;32m    120\u001b[0m       \u001b[0minitializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m     \u001b[0minit_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m     \u001b[0mvariable_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0muse_resource\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/zhen_gpu/lib/python3.6/site-packages/tensorflow/python/ops/init_ops_v2.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, shape, dtype)\u001b[0m\n\u001b[1;32m    556\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m       \u001b[0mlimit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_random_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_uniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/zhen_gpu/lib/python3.6/site-packages/tensorflow/python/ops/init_ops_v2.py\u001b[0m in \u001b[0;36mrandom_uniform\u001b[0;34m(self, shape, minval, maxval, dtype)\u001b[0m\n\u001b[1;32m   1066\u001b[0m       \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_uniform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m     return op(\n\u001b[0;32m-> 1068\u001b[0;31m         shape=shape, minval=minval, maxval=maxval, dtype=dtype, seed=self.seed)\n\u001b[0m\u001b[1;32m   1069\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtruncated_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstddev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/zhen_gpu/lib/python3.6/site-packages/tensorflow/python/ops/random_ops.py\u001b[0m in \u001b[0;36mrandom_uniform\u001b[0;34m(shape, minval, maxval, dtype, seed, name)\u001b[0m\n\u001b[1;32m    299\u001b[0m           \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmaxval\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mminval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m     \u001b[0;31m# TODO(b/132092188): C++ shape inference inside functional ops does not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;31m# cross FuncGraph boundaries since that information is only available in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/zhen_gpu/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/zhen_gpu/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6651\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6652\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6653\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6654\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/zhen_gpu/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[3744,4096] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Add]"
     ]
    }
   ],
   "source": [
    "model_relu_sgd = form_model(ativa = \"relu\",optima='sgd')\n",
    "model_relu_sgd = train_model(model_relu_sgd,'_3relu_sgd_xdviolence',weights_path_zu)\n",
    "\n",
    "model_gelu_adam = form_model(ativa = \"gelu\",optima='adam')\n",
    "model_gelu_adam = train_model(model_gelu_adam,'_3gelu_adam_xdviolence',weights_path_zu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict_total_max, predict_total = test_model(model=model_gelu,files=test_fn,rslt_path=rslt_path_zu,model_weight_fn=weights_fn[0].replace('.h5',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#res_list_full, res_list_max, res_list_fn, res_list_labels = get_results_from_txt(rslt_path=rslt_path_zu)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zhen_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6eb85c0477d574fd6bdabb52dbe9212bb7f487155853edb797b76ac4297f2c9e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
