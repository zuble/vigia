{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf 2.2.0\n",
      "cudatoolkit               10.1.243            h8cb64d8_11    conda-forge\n",
      "cudnn                     7.6.5.32             hc0a50b0_1    conda-forge\n",
      "neptune-tensorflow-keras  2.1.0              pyhd8ed1ab_0    conda-forge\n",
      "numpy                     1.19.5           py38hc896f84_4  \n",
      "numpy-base                1.19.5           py38h21a3de8_4  \n",
      "tensorflow                2.2.0           gpu_py38hb782248_0    anaconda\n",
      "tensorflow-base           2.2.0           gpu_py38h83e3d50_0    anaconda\n",
      "tensorflow-estimator      2.6.0            py38h709712a_0    conda-forge\n",
      "tensorflow-gpu            2.2.0                h0d30ee6_0    anaconda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtstudents/anaconda3/envs/zugpu/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/jtstudents/anaconda3/envs/zugpu/lib/python3.8/site-packages/neptune/internal/backends/hosted_client.py:48: NeptuneDeprecationWarning: The 'neptune-client' package has been deprecated and will be removed in the future. Install the 'neptune' package instead. For more, see https://docs.neptune.ai/setup/upgrading/\n",
      "  from neptune.version import version as neptune_client_version\n"
     ]
    }
   ],
   "source": [
    "import os, time, random, logging\n",
    "import cv2\n",
    "import numpy as np\n",
    "import PySimpleGUI as sg\n",
    "from pathlib import Path\n",
    "import csv\n",
    "#import mtcnn\n",
    "\n",
    "#import pandas as pd\n",
    "import tensorflow as tf\n",
    "print(\"tf\",tf.version.VERSION)\n",
    "#os.system(\"cat /usr/local/cuda/version.txt\")\n",
    "#os.system(\"nvcc --version\\n\")\n",
    "os.system(\"conda list | grep -E 'tensorflow|cudnn|cudatoolkit|numpy'\")\n",
    "\n",
    "#from tensorflow import keras\n",
    "#from keras import backend as K\n",
    "\n",
    "#from keras import models, layers, backend as K\n",
    "#from keras.layers import Activation\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "#from tensorflow.keras.utils import get_custom_objects\n",
    "#from keras.utils.generic_utils import get_custom_objects\n",
    "from tqdm.keras import TqdmCallback\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import neptune\n",
    "from neptune.integrations.tensorflow_keras import NeptuneCallback\n",
    "\n",
    "import utils.auxua as aux\n",
    "import utils.tf_formh5 as tf_formh5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' GPU CONFIGURATION '''\n",
    "\n",
    "tf_formh5.set_tf_loglevel(logging.ERROR)\n",
    "tf_formh5.tf.debugging.set_log_device_placement(False) #Enabling device placement logging causes any Tensor allocations or operations to be printed.\n",
    "tf_formh5.set_memory_growth()\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' NEPTUNE '''\n",
    "#https://docs.neptune.ai/integrations/keras/\n",
    "\n",
    "with open('/raid/DATASETS/.zuble/.nept', 'r') as file:nept = file.read()\n",
    "run = neptune.init_run( api_token=nept, project=\"vigia/base\")\n",
    "project = neptune.init_project(project=\"vigia/base\", api_token=nept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\" TEST & TRAIN FILES \"\"\"\n",
    "\n",
    "def test_files(onev = 0):\n",
    "    \"\"\"\n",
    "    GENERATE LIST of train FILES\n",
    "    \"\"\"\n",
    "    test_fn, test_normal_fn, test_abnormal_fn = [],[],[]\n",
    "    test_labels, test_normal_labels, test_abnormal_labels = [],[],[]\n",
    "    \n",
    "    \n",
    "    #makes sure that nept log are clear\n",
    "    try:del run[\"test/data_info\"]\n",
    "    except:run[\"test/data_info\"]\n",
    "\n",
    "    #all test files\n",
    "    if onev == 0:\n",
    "        for root, dirs, files in os.walk(aux.SERVER_TEST_PATH):\n",
    "            for file in files:\n",
    "                if file.find('.mp4') != -1:\n",
    "                    if 'label_A' in file:\n",
    "                        test_normal_fn.append(os.path.join(root, file))\n",
    "                        test_normal_labels.append(0)\n",
    "                        run[\"test/data_info/test_normal\"].append(str((file,0)))\n",
    "                    else:\n",
    "                        test_abnormal_fn.append(os.path.join(root, file))\n",
    "                        test_abnormal_labels.append(1)    \n",
    "                        run[\"test/data_info/test_abnormal\"].append(str((file,1)))          \n",
    "\n",
    "        test_labels = test_normal_labels + test_abnormal_labels                \n",
    "        test_fn = test_normal_fn + test_abnormal_fn\n",
    "        for i in range(len(test_fn)): run[\"test/data_info/test\"].append(test_fn[i])\n",
    "        \n",
    "    #only onev random files\n",
    "    else :\n",
    "        test_abn_fn = [x for x in os.listdir(aux.SERVER_TEST_PATH) if 'label_A' not in x]\n",
    "        test_nor_fn = [x for x in os.listdir(aux.SERVER_TEST_PATH) if 'label_A' in x]\n",
    "        \n",
    "        onev_abnor = int(onev/2)\n",
    "        while True: \n",
    "            ap = random.choice(test_abn_fn) \n",
    "            if ap not in test_fn: \n",
    "                test_fn.append(aux.SERVER_TEST_PATH+\"/\"+ap)\n",
    "                test_labels.append(1)\n",
    "                if len(test_fn) == onev_abnor: \n",
    "                    break \n",
    "        while True: \n",
    "            ap = random.choice(test_nor_fn) \n",
    "            if ap not in test_fn: \n",
    "                test_fn.append(aux.SERVER_TEST_PATH+\"/\"+ap)\n",
    "                test_labels.append(0)\n",
    "                if len(test_fn) == onev: \n",
    "                    break    \n",
    "    \n",
    "    \n",
    "    run[\"test/data_info/test_shape\"] = \"total_fn \"+str(np.shape(test_fn)[0])+\"\\ntotal_labels \"+str(np.shape(test_labels)[0])+\\\n",
    "                                        \"\\nnormal_fn \"+str(np.shape(test_normal_fn)[0])+\"\\nnormal_labels \"+str(np.shape(test_normal_labels)[0])+\\\n",
    "                                        \"\\nabnormal_fn \"+str(np.shape(test_abnormal_fn)[0])+\"\\nabnormal_labels \"+str(np.shape(test_abnormal_labels)[0])\n",
    "    \n",
    "    print(\"\\ntest_fn\",np.shape(test_fn),\"\\ntest_normal_fn\",np.shape(test_normal_fn),\"\\ntest_abnormal_fn\",np.shape(test_abnormal_fn))\n",
    "    print(\"\\ntest_labels\",np.shape(test_labels),\"\\ntest_normal_labels\",np.shape(test_normal_labels),\"\\ntest_abnormal_labels\",np.shape(test_abnormal_labels))\n",
    "    print('\\n-------------------')\n",
    "    return test_fn , test_normal_fn , test_abnormal_fn , test_labels \n",
    "\n",
    "def train_valdt_files():\n",
    "    \"\"\"\n",
    "    GENERATING LIST of TRAIN FILES\n",
    "    \"\"\"\n",
    "    full_train_fn, full_train_normal_fn, full_train_abnormal_fn = [],[],[]\n",
    "    full_train_labels, full_train_normal_labels, full_train_abnormal_labels = [],[],[]\n",
    "\n",
    "    #makes sure that neptlog are clear\n",
    "    try:del run[\"train/data_info\"]\n",
    "    except:run[\"train/data_info\"]\n",
    "\n",
    "    for root, dirs, files in os.walk(aux.SERVER_TRAIN_PATH):\n",
    "        for file in files:\n",
    "            if file.find('.mp4') != -1:\n",
    "                full_train_fn.append(os.path.join(root, file))\n",
    "                run[\"train/data_info/full_train\"].append(file)\n",
    "\n",
    "                if 'label_A' in file:\n",
    "                    full_train_normal_fn.append(os.path.join(root, file))\n",
    "                    full_train_normal_labels.append(0)\n",
    "                    run[\"train/data_info/full_train_normal\"].append(str((file,0)))\n",
    "\n",
    "                else:\n",
    "                    full_train_abnormal_fn.append(os.path.join(root, file))\n",
    "                    full_train_abnormal_labels.append(1)\n",
    "                    run[\"train/data_info/full_train_abnormal\"].append(str((file,1)))\n",
    "    #BEFORE SPLIT INTO TRAIN+VALD\n",
    "    run[\"train/data_info/full_train_shape\"] = str(\"total \"+str(np.shape(full_train_fn)[0])+\"\\nabnormal \"+str(np.shape(full_train_abnormal_fn)[0])+\"\\nnormal \"+str(np.shape(full_train_normal_fn)[0]))\n",
    "    print(\"\\nfull_train_fn\",np.shape(full_train_fn),\"\\nfull_train_normal_fn\",np.shape(full_train_normal_fn),\"\\nfull_train_abnormal\",np.shape(full_train_abnormal_fn))\n",
    "    \n",
    "\n",
    "    #AFTER SPLIT\n",
    "    valdt_fn, valdt_normal_fn, valdt_abnormal_fn = [],[],[]\n",
    "    valdt_labels, valdt_normal_labels, valdt_abnormal_labels = [],[],[]\n",
    "\n",
    "    train_fn, train_normal_fn, train_abnormal_fn = [],[],[]\n",
    "    train_labels, train_normal_labels, train_abnormal_labels = [],[],[]\n",
    "\n",
    "    train_fn, valdt_fn = train_test_split(full_train_fn, test_size=0.2,shuffle=False)\n",
    "    for i in range(len(train_fn)):\n",
    "        if 'label_A' in train_fn[i]:train_normal_fn.append(train_fn[i]);train_normal_labels.append(0);train_labels.append(0)\n",
    "        else: train_abnormal_fn.append(train_fn[i]);train_abnormal_labels.append(1);train_labels.append(1)\n",
    "    \n",
    "    run[\"train/data_info/train_shape\"] = str(\"total \"+str(np.shape(train_fn)[0])+\"\\nabnormal \"+str(np.shape(train_abnormal_fn)[0])+\"\\nnormal \"+str(np.shape(train_normal_fn)[0]))\n",
    "    print(\"\\ntrain_fn\",np.shape(train_fn),\"\\ntrain_normal_fn\",np.shape(train_normal_fn),\"\\ntrain_abnormal_fn\",np.shape(train_abnormal_fn))\n",
    "    \n",
    "    \n",
    "    for i in range(len(valdt_fn)):\n",
    "        if 'label_A' in valdt_fn[i]:valdt_normal_fn.append(valdt_fn[i]);valdt_normal_labels.append(0);valdt_labels.append(0)\n",
    "        else: valdt_abnormal_fn.append(valdt_fn[i]);valdt_abnormal_labels.append(1);valdt_labels.append(1)   \n",
    "    \n",
    "    run[\"train/data_info/valdt_shape\"] = str(\"total \"+str(np.shape(valdt_fn)[0])+\"\\nabnormal \"+str(np.shape(valdt_abnormal_fn)[0])+\"\\nnormal \"+str(np.shape(valdt_normal_fn)[0]))\n",
    "    print(\"\\nvaldt_fn\",np.shape(valdt_fn),\"\\nvaldt_normal_fn\",np.shape(valdt_normal_fn),\"\\nvaldt_abnormal_fn\",np.shape(valdt_abnormal_fn))\n",
    "\n",
    "    return train_fn, train_labels, valdt_fn, valdt_labels\n",
    "\n",
    "def nept_load_dataset():\n",
    "    \n",
    "    #run[\"dataset/train\"].track_files(aux.SERVER_TRAIN_PATH,wait=True)\n",
    "    \n",
    "    \n",
    "    del project[\"dataset\"]\n",
    "    \n",
    "    for i in range(len(train_normal_fn)): project[\"dataset/train_normal/\"+os.path.basename(train_normal_fn[i])].upload(train_normal_fn[i])\n",
    "    for i in range(len(train_abnormal_fn)): project[\"dataset/train_abnormal/\"+os.path.basename(train_normal_fn[i])].upload(train_abnormal_fn[i])\n",
    "    \n",
    "    for i in range(len(test_normal_fn)): project[\"dataset/test_normal/\"+os.path.basename(train_normal_fn[i])].upload(test_normal_fn[i])\n",
    "    for i in range(len(test_abnormal_fn)): project[\"dataset/test_abnormal/\"+os.path.basename(train_normal_fn[i])].upload(test_abnormal_fn[i])\n",
    "    \n",
    "\n",
    "test_fn , test_abnormal_fn , test_normal_fn , test_labels = test_files()\n",
    "train_fn, train_labels, valdt_fn, valdt_labels = train_valdt_files()\n",
    "\n",
    "update_index_train = range(0, len(train_fn))\n",
    "update_index_valdt = range(0, len(valdt_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" INPUT DATA\"\"\"\n",
    "\n",
    "in_height = 120; in_width = 160\n",
    "\n",
    "def input_train_video_data(file_name):\n",
    "    print(\"\\n\\ninput_train_video_data\\n\")\n",
    "    #file_name = 'C:\\\\Bosch\\\\Anomaly\\\\training\\\\videos\\\\13_007.avi'\n",
    "    #file_name = '/raid/DATASETS/anomaly/UCF_Crimes/Videos/Training_Normal_Videos_Anomaly/Normal_Videos308_x264.mp4'\n",
    "    video = cv2.VideoCapture(file_name)\n",
    "    total_frame = video.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    #mtcnn_detector = mtcnn.mtcnn.MTCNN()\n",
    "    #print(file_name + '  ' + str(total_frame))\n",
    "    divid_no = 1\n",
    "    \n",
    "    frame_max = train_config[\"frame_max\"]\n",
    "    \n",
    "    # define the nmbers of batchs to divid atual video (divid_no)\n",
    "    if total_frame > int(frame_max):\n",
    "        total_frame_int = int(total_frame)\n",
    "        if total_frame_int % int(frame_max) == 0:\n",
    "            divid_no = int(total_frame / int(frame_max))\n",
    "        else:\n",
    "            divid_no = int(total_frame / int(frame_max)) + 1\n",
    "        \n",
    "    batch_no = 0\n",
    "    batch_frames = []\n",
    "    batch_frames_flip = []\n",
    "    counter = 0\n",
    "    \n",
    "    # gets random batch w\\ frame max lenght \n",
    "    if 'Normal' in file_name:\n",
    "        print(\"\\n\\nNORMAL\\n\\n\")\n",
    "        if divid_no != 1:\n",
    "            slice_no = int(random.random()*divid_no)\n",
    "            passby = 0\n",
    "            if slice_no != divid_no - 1:\n",
    "                while video.isOpened and passby < int(frame_max) * slice_no:\n",
    "                    passby += 1\n",
    "                    success, image = video.read()\n",
    "                    if success == False:\n",
    "                        break\n",
    "            else:\n",
    "                while video.isOpened and passby < total_frame - int(frame_max):\n",
    "                    passby += 1\n",
    "                    success, image = video.read()\n",
    "                    if success == False:\n",
    "                        break\n",
    "\n",
    "    while video.isOpened:               \n",
    "        success, image = video.read()\n",
    "        if success == False:\n",
    "            break\n",
    "            \n",
    "        #ratio = image.shape[0] / image.shape[1]\n",
    "        #print(str(image.shape[0])+ ' ' + str(image.shape[1]))\n",
    "        #image = cv2.resize(image, (800, int(800*ratio)))\n",
    "        #print(image.shape)\n",
    "        #faces = face_detector.detectMultiScale(image,1.1,8)\n",
    "        '''\n",
    "        faces = mtcnn_detector.detect_faces(image)\n",
    "        \n",
    "        for face in faces:\n",
    "            (x,y,w,h) = face['box']\n",
    "            #print(face)\n",
    "            cv2.rectangle(image,(x,y), (x+w,y+h), (255,255,0), 2)\n",
    "            cv2.putText(image, str(face['confidence'])[:4], (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (36,255,12), 1)\n",
    "        '''\n",
    "        image = cv2.resize(image, (in_width, in_height))\n",
    "        image_flip = cv2.flip(image, 1)\n",
    "        \n",
    "        image_array = np.array(image)/255.0\n",
    "        image_array_flip = np.array(image_flip)/255.0\n",
    "        \n",
    "        batch_frames.append(image_array)\n",
    "        batch_frames_flip.append(image_array_flip)\n",
    "        \n",
    "        counter += 1\n",
    "        if counter > int(frame_max):\n",
    "            break\n",
    "            \n",
    "    video.release()\n",
    "    batch_frames = np.array(batch_frames)\n",
    "    #print(batch_frames.shape)\n",
    "        \n",
    "    return np.expand_dims(batch_frames,0), np.expand_dims(batch_frames_flip, 0), total_frame\n",
    "\n",
    "def generate_input(data,update_index,validation):\n",
    "    #has_visited = [0 for i in range(len(train_fn))]\n",
    "    data_var_name = [k for k, v in globals().items() if v is data][0]\n",
    "    print(\"\\n\\nGENERATE_INPUT FOR\",data_var_name,\\\n",
    "        '\\n\\tupdate_index len = ',len(update_index),\\\n",
    "        '\\n\\tdata len = ',len(data))\n",
    "    \n",
    "    loop_no = 0\n",
    "    while 1:\n",
    "        index = update_index[loop_no]\n",
    "        loop_no += 1\n",
    "        print(\"\\n\",data_var_name,\" index\",index,\" loop_no\",loop_no)\n",
    "        if loop_no == len(data):loop_no= 0\n",
    "        \n",
    "        #index = 0\n",
    "        batch_frames, batch_frames_flip, total_frames = input_train_video_data(data[index])\n",
    "        print(\"\\n\\t\",data_var_name,\"data[\",index,\"]=\",data[index],\"\\n\\ttotal_frames=\",total_frames,\"\\n\\tbatch_frames.shape=\",batch_frames.shape,\"\\n\")\n",
    "        #if batch_frames.ndim != 5:\n",
    "        #   break\n",
    "        \n",
    "        # GENERATORS    \n",
    "        #       a kind of iterators that can only iterate over once\n",
    "        #       NO store of values in memory\n",
    "        # YIELD \n",
    "        #       like a return, except the function will return a generator\n",
    "        \n",
    "        if not validation:\n",
    "            #batch_frames\n",
    "            if 'label_A' in data[index]: yield batch_frames, np.array([0])   #normal\n",
    "            else: yield batch_frames, np.array([1])   #abnormal\n",
    "            \n",
    "            #batch_frames_flip\n",
    "            if 'label_A' in data[index]: yield batch_frames_flip, np.array([0])  #normal\n",
    "            else: yield batch_frames_flip, np.array([1])  #abnormal\n",
    "        else:\n",
    "            #batch_frames\n",
    "            if 'label_A' in data[index]: yield batch_frames, np.array([0])   #normal\n",
    "            else: yield batch_frames, np.array([1])   #abnormal\n",
    "                \n",
    "    print(\"\\nloop_no=\",loop_no)\n",
    "\n",
    "\n",
    "def input_test_video_data(file_name,config,batch_no=0):\n",
    "    #file_name = 'C:\\\\Bosch\\\\Anomaly\\\\training\\\\videos\\\\13_007.avi'\n",
    "    #file_name = '/raid/DATASETS/anomaly/UCF_Crimes/Videos/Training_Normal_Videos_Anomaly/Normal_Videos308_x264.mp4'\n",
    "    video = cv2.VideoCapture(file_name)\n",
    "    total_frame = video.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    fps = video.get(cv2.CAP_PROP_FPS)\n",
    "    #mtcnn_detector = mtcnn.mtcnn.MTCNN()\n",
    "    divid_no = 1\n",
    "    frame_max = config[\"frame_max\"]\n",
    "    batch_type = config[\"batch_type\"]\n",
    "    \n",
    "    # define the nmbers of batchs to divid atual video (divid_no)\n",
    "    if total_frame > int(frame_max):\n",
    "        total_frame_int = int(total_frame)\n",
    "        if total_frame_int % int(frame_max) == 0:\n",
    "            divid_no = int(total_frame / int(frame_max))\n",
    "        else:\n",
    "            divid_no = int(total_frame / int(frame_max)) + 1\n",
    "\n",
    "\n",
    "    #updates the start frame to 0,frame_max*1,frame_max*2... excluding the last batch\n",
    "    passby = 0\n",
    "    if batch_no != divid_no - 1:\n",
    "        while video.isOpened and passby < int(frame_max) * batch_no:\n",
    "            passby += 1\n",
    "            success, image = video.read()\n",
    "            if success == False:\n",
    "                break\n",
    "            \n",
    "    #updates the last batch starting frame \n",
    "    else:\n",
    "        if batch_type==1:\n",
    "            #print(\"1\")\n",
    "            while video.isOpened and passby < total_frame - int(frame_max):\n",
    "                passby += 1\n",
    "                success, image = video.read()\n",
    "                if success == False:\n",
    "                    break\n",
    "        #last batch must have >= frame_max/10 otherwise it falls back to batch_type 1\n",
    "        if batch_type==2 and total_frame - (int(frame_max) * batch_no) >= int(frame_max)*0.1:\n",
    "            #print(\"2\")\n",
    "            while video.isOpened and passby < int(frame_max) * batch_no:\n",
    "                passby += 1\n",
    "                success, image = video.read()\n",
    "                if success == False:\n",
    "                    break\n",
    "        else:\n",
    "            while video.isOpened and passby < total_frame - int(frame_max):\n",
    "                passby += 1\n",
    "                success, image = video.read()\n",
    "                if success == False:\n",
    "                    break\n",
    "\n",
    "            \n",
    "    batch_frames, batch_imgs = [], []\n",
    "    counter = 0\n",
    "    \n",
    "    while video.isOpened:               \n",
    "        success, image = video.read()\n",
    "        if success == False:\n",
    "            break\n",
    "        batch_imgs.append(image)\n",
    "        \n",
    "        image_rsz = cv2.resize(image, (in_width, in_height))\n",
    "        image_array = np.array(image_rsz)/255.0 #normalize\n",
    "        batch_frames.append(image_array)\n",
    "        \n",
    "        counter += 1\n",
    "        if counter > int(frame_max):\n",
    "            break\n",
    "            \n",
    "    video.release()\n",
    "    \n",
    "    #batch_frames_tensor = tf.convert_to_tensor(batch_frames)\n",
    "    ##print(\"\\tshap tensor\",tf.shape( tf.expand_dims(batch_frames_tensor,0) ) )\n",
    "    #print(\"\\t-batch\",batch_no,\"[\",passby,\", ... ] \", batch_frames_tensor.get_shape().as_list() )    \n",
    "\n",
    "    batch_frames = np.array(batch_frames)\n",
    "    #print(\"\\tshap NP ARRAY\",np.shape( np.expand_dims(batch_frames,0) ))\n",
    "    print(\"\\t-batch\",batch_no,\"[\",passby,\", ... ] \",batch_frames.shape)    \n",
    "\n",
    "    #return tf.expand_dims(batch_frames_tensor,0), batch_imgs, divid_no, total_frame, passby, fps\n",
    "    return np.expand_dims(batch_frames,0), batch_imgs, divid_no, total_frame, passby, fps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' TRAIN FX '''\n",
    "\n",
    "def train_model(model,config,ckptgui=False):\n",
    "    '''\n",
    "    MODEL TRAIN/VALIDATION \n",
    "    (silent mode - verbose = 0)\n",
    "    '''\n",
    "    print(\"\\nTRAIN MODEL\")\n",
    "    \n",
    "    #start from ckpt .h5\n",
    "    if int(config[\"ckpt_start\"]):  #aux = f\"{34:0>8}\"; if int(aux): print(type(aux), aux)\n",
    "        if ckptgui:\n",
    "            model_h5ckpt, model_h5ckpt_path = tf_formh5.find_h5(aux.CKPT_PATH,find_string=(),ruii=True)\n",
    "            model_h5ckpt = os.path.splitext(model_h5ckpt)[0]\n",
    "            model.load_weights(model_h5ckpt_path)\n",
    "        else:\n",
    "            find_string=[config[\"ativa\"]+'_'+config[\"optima\"]+'_'+str(config[\"batch_type\"])+'_'+config[\"frame_max\"],config[\"ckpt_start\"]]\n",
    "            model_h5ckpt, model_h5ckpt_path = tf_formh5.find_h5(aux.CKPT_PATH,find_string,ruii=False)\n",
    "\n",
    "            model.load_weights(model_h5ckpt_path[0])\n",
    "            print(\"\\n\\tWEIGHTS from ckpt\", '/'+os.path.split(os.path.split(model_h5ckpt_path[0])[0])[1]+'/'+os.path.split(model_h5ckpt_path[0])[1])\n",
    "            \n",
    "        model_name = model_h5ckpt[0]\n",
    "        run[\"train/model_name\"] = model_name\n",
    "        \n",
    "        # ckeck if its necessary to create a ckpt folder , else check is empty\n",
    "        ckpt_path_nw = aux.CKPT_PATH+model_name\n",
    "        if os.path.exists(ckpt_path_nw):\n",
    "            if len(os.listdir(ckpt_path_nw)) == 0:print(\"\\n\\tCKPTs at \",ckpt_path_nw)\n",
    "            else: raise Exception(f\"{ckpt_path_nw} is not empty\")\n",
    "        else:os.makedirs(ckpt_path_nw);print(\"\\n\\tCKPTs created at \",ckpt_path_nw)\n",
    "        \n",
    "        run[\"train/path_ckpt\"] = ckpt_path_nw\n",
    "        checkpoint_callback = ModelCheckpoint(filepath=ckpt_path_nw+'/'+model_name+'-{epoch:08d}.h5') #https://keras.io/api/callbacks/model_checkpoint/\n",
    "\n",
    "    #start from zero\n",
    "    else:\n",
    "        time_str = str(time.time()); \n",
    "        model_name = time_str + '_'+config[\"ativa\"]+'_'+config[\"optima\"]+'_'+str(config[\"batch_type\"])+'_'+config[\"frame_max\"]\n",
    "        run[\"train/model_name\"] = model_name\n",
    "\n",
    "        ckpt_path_nw = aux.CKPT_PATH+model_name\n",
    "        if not os.path.exists(ckpt_path_nw):\n",
    "            os.makedirs(ckpt_path_nw)\n",
    "        else:raise Exception(f\"{ckpt_path_nw} eristes\")\n",
    "        \n",
    "        checkpoint_callback = ModelCheckpoint(filepath=ckpt_path_nw+'/'+model_name+'_ckpt-{epoch:08d}.h5') #https://keras.io/api/callbacks/model_checkpoint/\n",
    "        \n",
    "        print(\"\\n\\tCKPTs at \",ckpt_path_nw)\n",
    "        run[\"train/path_ckpt\"] = ckpt_path_nw\n",
    "        \n",
    " \n",
    "    print(\"\\n\\tMODEL.FIT w/ name \",model_name)\n",
    "    #early_stop_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=4)\n",
    "    neptune_callback = NeptuneCallback(run=run,log_on_batch=True,log_model_diagram=True) \n",
    "    history = model.fit(generate_input(data = train_fn,update_index=update_index_train,validation=False), \n",
    "                        steps_per_epoch=len(train_fn)*2,\n",
    "                        epochs=config[\"epochs\"], \n",
    "                        verbose=2,\n",
    "                        validation_data=generate_input(data=valdt_fn,update_index=update_index_valdt,validation=True),\n",
    "                        validation_steps=len(valdt_fn),\n",
    "                        callbacks=[checkpoint_callback,  \\\n",
    "                                   #early_stop_callback, \\\n",
    "                                   TqdmCallback(verbose=0), \\\n",
    "                                   neptune_callback])\n",
    "    \n",
    "    model.save(aux.MODEL_PATH + model_name + '.h5')\n",
    "    model.save(aux.MODEL_PATH + model_name )\n",
    "    run[\"train/path_model\"]=aux.MODEL_PATH+model_name\n",
    "\n",
    "    model.save_weights(aux.WEIGHTS_PATH + model_name + '_weights.h5')\n",
    "    run[\"train/path_weights\"]=aux.WEIGHTS_PATH+model_name+'_weights.h5' \n",
    "    \n",
    "    # Save the history to a CSV file\n",
    "    hist_csv_file = aux.HIST_PATH + model_name + '_history.csv'\n",
    "    with open(hist_csv_file, 'w', newline='') as file:writer = csv.writer(file);writer.writerow(history.history.keys());writer.writerows(zip(*history.history.values()))\n",
    "    # OR\n",
    "    #hist_df = pd.DataFrame(history.history)\n",
    "    #with open(hist_csv_file, mode = 'w') as f:hist_df.to_csv(f)\n",
    "    \n",
    "    run[\"train/model_hist_csv_file\"].upload(hist_csv_file)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' TEST FX '''\n",
    "\n",
    "@tf.function\n",
    "def as_predict(model,x):\n",
    "    return model(x, training=False)    \n",
    "\n",
    "def test_model(model,model_name,config,files=test_fn):\n",
    "    print(\"\\n\\nTEST MODEL\\n\")\n",
    "\n",
    "    # rslt txt file creation\n",
    "    txt_path = aux.RSLT_PATH+model_name+'-'+str(config[\"batch_type\"])+'_'+str(config[\"frame_max\"])+'.txt'\n",
    "    if os.path.isfile(txt_path):raise Exception(txt_path,\"eriste\")\n",
    "    else: print(\"\\tSaving @\",txt_path,\"\\n\");run[\"test/path_rslt\"] = txt_path\n",
    "    \n",
    "    f = open(txt_path, 'w')\n",
    "    \n",
    "    content_str = ''\n",
    "    total_frames_test = 0\n",
    "    \n",
    "    predict_total = [] #to output predict in vizualizer accordingly to the each batch prediction\n",
    "    predict_max = 0     #to print the max predict related to the file in test\n",
    "    predict_total_max = [] #to perform the metrics\n",
    "    \n",
    "    start_test = time.time()\n",
    "    for i in range(len(files)):\n",
    "        if files[i] != '':\n",
    "            file_path = files[i]\n",
    "            predict_result = () #to save predictions per file\n",
    "            time_batch_predict = time_video_predict = 0.0\n",
    "\n",
    "            #the frist 4000 frames from actual test video                \n",
    "            batch_frames, batch_imgs, divid_no, total_frames,start_frame, fps = input_test_video_data(file_path,config)\n",
    "            video_time = total_frames/fps\n",
    "            total_frames_test += total_frames\n",
    "            \n",
    "            #prediction on frist batch\n",
    "            start_predict1 = time.time()\n",
    "            #predict_aux = model.predict(batch_frames)[0][0]\n",
    "            \n",
    "            predict_aux = as_predict(model,batch_frames)[0][0].numpy() #using tf.function\n",
    "            #predict_aux = model(batch_frames,training=False)[0][0]\n",
    "            end_predict1 = time.time()\n",
    "            time_video_predict = time_batch_predict = end_predict1-start_predict1\n",
    "            \n",
    "            predict_max = predict_aux\n",
    "            predict_result = (divid_no,start_frame+batch_frames.shape[1],predict_max)\n",
    "            #print(predict_result,batch_frames.shape)\n",
    "            \n",
    "            high_score_patch = 0\n",
    "            print(\"\\t \",predict_max,\"%\",\" in \",\"{:.4f}\".format(time_batch_predict),\" secs\")\n",
    "            \n",
    "            #when batch_frames (input video) has > frame_max frames\n",
    "            patch_num = 1\n",
    "            while patch_num < divid_no:\n",
    "                batch_frames, batch_imgs, divid_no, total_frames,start_frame, fps = input_test_video_data(file_path,config,patch_num)\n",
    "                \n",
    "                #nésimo batch prediction\n",
    "                start_predict2 = time.time()\n",
    "                #predict_aux = model.predict(batch_frames)[0][0]\n",
    "                predict_aux = as_predict(model,batch_frames)[0][0].numpy() #using tf.function\n",
    "                end_predict2 = time.time()\n",
    "                time_batch_predict = end_predict2 - start_predict2\n",
    "                time_video_predict += time_batch_predict\n",
    "\n",
    "                if predict_aux > predict_max:\n",
    "                    predict_max = predict_aux\n",
    "                    high_score_patch = patch_num\n",
    "                \n",
    "                predict_result += (start_frame,start_frame+batch_frames.shape[1], predict_aux)\n",
    "                #print(predict_result)\n",
    "                \n",
    "                print(\"\\t \",predict_aux,\"%\",\" in \",\"{:.4f}\".format(time_batch_predict),\" secs\")  \n",
    "                patch_num += 1\n",
    "            \n",
    "            predict_total.append(predict_result)\n",
    "            predict_total_max.append(predict_max)\n",
    "            print(\"\\n\\t\",predict_total[i])\n",
    "            \n",
    "            if 'label_A' in files[i]:\n",
    "                print('\\nNORM',str(i),':',f'{total_frames:.0f}',\"@\",f'{fps:.0f}',\"fps =\",f'{video_time:.2f}',\"sec\\n\\t\",\n",
    "                        files[i][files[i].rindex('/')+1:],\n",
    "                        \"\\n\\t \"+str(predict_max),\"% @batch\",high_score_patch,\"in\",str(time_video_predict),\"seconds\\n\",\n",
    "                        \"----------------------------------------------------\\n\")\n",
    "            else:\n",
    "                print('\\nABNORM',str(i),':',f'{total_frames:.0f}',\"@\",f'{fps:.0f}',\"fps =\",f'{video_time:.2f}',\"sec\\n\\t\",\n",
    "                        files[i][files[i].rindex('/')+1:],\n",
    "                        \"\\n\\t\"+str(predict_max),\"% @batch\",high_score_patch,\"in\",str(time_video_predict),\"seconds\\n\",\n",
    "                        \"----------------------------------------------------\\n\")\n",
    "                \n",
    "            content_str += files[i][files[i].rindex('/')+1:] + '|' + str(predict_total_max[i]) + '|' + str(predict_total[i])  + '\\n'\n",
    "            \n",
    "    end_test = time.time()\n",
    "    time_test = end_test - start_test\n",
    "\n",
    "    f.write(content_str)\n",
    "    f.close()\n",
    "    print(\"\\nDONE\\n\\ttotal of\",str(total_frames_test),\"frames processed in\",time_test,\" seconds\",\n",
    "            \"\\n\\t\"+str(total_frames_test / time_test),\"frames per second\",\n",
    "            \"\\n\\n********************************************************\",\n",
    "            \"\\n\\n********************************************************\")                  \n",
    "\n",
    "    #remove white spaces in file , for further easier reading\n",
    "    with open(txt_path, 'r+') as f:txt=f.read().replace(' ', '');f.seek(0);f.write(txt);f.truncate()\n",
    "    aux.sort_files(txt_path) #sort alphabetcly\n",
    "    run[\"test/model/rslt\"].upload(txt_path)\n",
    "    \n",
    "    return predict_total_max, predict_total\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''INIT TRAIN MODEL'''\n",
    "\n",
    "train_config = {\n",
    "    \"ativa\" : 'leakyrelu',\n",
    "    \"optima\" : 'adamamsgrad',\n",
    "    \"batch_type\" : 0,   # =0 all batch have frame_max or video length // =1 last batch has frame_max frames // =2 last batch has no repetead frames\n",
    "    \"frame_max\" : '4000',\n",
    "    \"ckpt_start\" : f\"{9:0>8}\",  #used in train_model: if 00000000 start from scratch, else start from ckpt with config stated\n",
    "    \"epochs\" : 21\n",
    "}\n",
    "#run[\"train/config_train\"].append(train_config)\n",
    "\n",
    "#model = tf_formh5.form_model(train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TRAIN \"\"\"\n",
    "\n",
    "#model = train_model(model,train_config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' INIT TEST MODEL '''\n",
    "\n",
    "wght4test_config = {\n",
    "    \"ativa\" : 'leakyrelu',\n",
    "    \"optima\" : 'adamamsgrad',\n",
    "    \"batch_type\" : 0, # =0 all batch have frame_max or video length // =1 last batch has frame_max frames // =2 last batch has no repetead frames\n",
    "    \"frame_max\" : '4000'\n",
    "}\n",
    "\n",
    "#run[\"test/config_wght4test\"].append(wght4test_config)\n",
    "\n",
    "model, model_name = tf_formh5.init_test_model(wght4test_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' TEST '''\n",
    "\n",
    "test_config = {\n",
    "    \"batch_type\" : 1, # =0 all batch have frame_max or video length // =1 last batch has frame_max frames // =2 last batch has no repetead frames\n",
    "    \"frame_max\" : '1000' \n",
    "}\n",
    "#run[\"test/config_test\"].append(test_config)\n",
    "\n",
    "predict_total_max, predict_total = test_model(model,model_name,test_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST ALL WEIGHTS\n",
    "#weights_names , weights_paths = h5_util.find_h5(aux.WEIGHTS_PATH,find_string=(''),ruii=False)\n",
    "#for j in range(len(weights_names)):print(weights_names[j])\n",
    "#for i in range(len(weights_paths)):\n",
    "#    #print(para_file_path[i])\n",
    "#    aux_load = weights_names[i].split(\"_\")\n",
    "#    if '3' in aux_load[1]:aux_load[1] = aux_load[1].strip('3') #for 3gelu\n",
    "#    if aux_load[4] == 'weights': aux_load[4] = '4000'\n",
    "#    #print(aux_load)\n",
    "#\n",
    "#    time_str = aux_load[0]\n",
    "#    ativa = aux_load[1]\n",
    "#    optima = aux_load[2]\n",
    "#    batch_type = aux_load[3]\n",
    "#    frame_max = aux_load[4]\n",
    "#\n",
    "#    load_info = (ativa,optima,'_'+str(batch_type)+'_',frame_max)\n",
    "#    print('\\n',load_info)\n",
    "#    \n",
    "#    model = form_model(load_info[0],load_info[1])\n",
    "#    predict_total_max, predict_total = test_model(model,load_info=load_info)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zugpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8f7bf267530dde59ab6764f0eb5f1b22dea8a9c4ca62db1182ae079b8b4c02bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
