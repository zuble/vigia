{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time, argparse, logging, math, gc, json\n",
    "\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import nd\n",
    "from mxnet.gluon.data.vision import transforms\n",
    "from gluoncv.data.transforms import video\n",
    "from gluoncv.model_zoo import get_model\n",
    "from gluoncv.data import VideoClsCustom\n",
    "from gluoncv.utils.filesystem import try_import_decord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.setup as setup\n",
    "from utils.vid import viewer , gerador_clips\n",
    "\n",
    "parser = argparse.ArgumentParser(description='fext')\n",
    "parser.add_argument('--experiment', \n",
    "                    type=str, \n",
    "                    default='cfg/xdviol0.yml', \n",
    "                    help='relative path to experiment .yml')\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "cfg = setup.init(args)\n",
    "from  utils.log import get_logger\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob , os\n",
    "dir = cfg.DATA_DIR\n",
    "sub_dirs = glob.glob(dir+\"/*_copy\")\n",
    "print(sub_dirs)\n",
    "\n",
    "for sub_dir in sub_dirs:\n",
    "    print(sub_dir.split(\"/\")[-1])\n",
    "    vpaths = glob.glob(sub_dir+\"/*.mp4\")\n",
    "    print(len(vpaths))\n",
    "#paths = glob.glob(dir+\"/*.mp4\")\n",
    "#paths.sort()\n",
    "#paths = paths\n",
    "#print(f'from {dir} got {len(paths)} vids')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu\n",
    "gc.set_threshold(100, 5, 5)\n",
    "\n",
    "# set env\n",
    "if cfg.GPUID[0] == -1: context = mx.cpu()\n",
    "else: \n",
    "    context = []\n",
    "    for gpu in cfg.GPUID:context.append(mx.gpu(gpu))\n",
    "print(\"context\",str(context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data preprocess\n",
    "image_norm_mean = [0.485, 0.456, 0.406]\n",
    "image_norm_std = [0.229, 0.224, 0.225]\n",
    "if cfg.TRANSFORM.TEN_CROP:\n",
    "    transform_test = transforms.Compose([\n",
    "        video.VideoTenCrop(cfg.TRANSFORM.INPUT_SIZE),\n",
    "        video.VideoToTensor(),\n",
    "        video.VideoNormalize(image_norm_mean, image_norm_std)\n",
    "    ])\n",
    "    cfg.DATA.NUM_CROP = 10\n",
    "elif cfg.TRANSFORM.THREE_CROP:\n",
    "    transform_test = transforms.Compose([\n",
    "        video.VideoThreeCrop(cfg.TRANSFORM.INPUT_SIZE),\n",
    "        video.VideoToTensor(),\n",
    "        video.VideoNormalize(image_norm_mean, image_norm_std)\n",
    "    ])\n",
    "    cfg.DATA.NUM_CROP = 3\n",
    "else:\n",
    "    transform_test = video.VideoGroupValTransform(size=cfg.TRANSFORM.INPUT_SIZE, mean=image_norm_mean, std=image_norm_std)\n",
    "    cfg.DATA.NUM_CROP = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model\n",
    "if cfg.MODEL.USE_PRETRAINED and len(cfg.MODEL.HASHTAG) > 0:\n",
    "    cfg.MODEL.USE_PRETRAINED = cfg.MODEL.HASHTAG\n",
    "classes = cfg.MODEL.NUM_CLASSES\n",
    "model_name = cfg.MODEL.NAME\n",
    "net = get_model(name=model_name, nclass=classes, pretrained=cfg.MODEL.USE_PRETRAINED,\n",
    "                feat_ext=True, num_segments=cfg.DATA.NUM_SEGMENTS, num_crop=cfg.DATA.NUM_CROP)\n",
    "net.cast(cfg.MODEL.DTYPE)\n",
    "#net.collect_params().reset_ctx(context)\n",
    "net.collect_params().initialize(force_reinit=True, ctx=devices)\n",
    "#print(net.collect_params())\n",
    "\n",
    "if cfg.MODEL.MODE == 'hybrid':\n",
    "    net.hybridize(static_alloc=True, static_shape=True)\n",
    "if cfg.MODEL.RESUME_PARAMS != '' and not cfg.MODEL.USE_PRETRAINED:\n",
    "    net.load_parameters(cfg.MODEL.RESUME_PARAMS, ctx=devices)\n",
    "    logger.info('Pre-trained model %s is successfully loaded.' % (cfg.MODEL.RESUME_PARAMS))\n",
    "else: logger.info('Pre-trained model is successfully loaded from the model zoo.')\n",
    "\n",
    "logger.info(\"Successfully built model {}\".format(model_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "f = open(cfg.DATA_LIST, 'r')\n",
    "data_list = f.readlines()\n",
    "logger.info('Load %d video samples.' % len(data_list))\n",
    "\n",
    "# build a pseudo dataset instance to use its children class methods\n",
    "video_utils = VideoClsCustom(root=cfg.DATA_DIR,\n",
    "                            setting=cfg.DATA_LIST,\n",
    "                            num_segments=cfg.DATA.NUM_SEGMENTS,\n",
    "                            num_crop=cfg.DATA.NUM_CROP,\n",
    "                            new_length=cfg.DATA.NEW_LENGTH,\n",
    "                            new_step=cfg.DATA.NEW_STEP,\n",
    "                            new_width=cfg.DATA.NEW_WIDTH,\n",
    "                            new_height=cfg.DATA.NEW_HEIGHT,\n",
    "                            video_loader=True,\n",
    "                            use_decord=True,\n",
    "                            slowfast=cfg.DATA.SLOWFAST,\n",
    "                            slow_temporal_stride=cfg.DATA.SLOW_TEMPORAL_STRIDE,\n",
    "                            fast_temporal_stride=cfg.DATA.FAST_TEMPORAL_STRIDE,\n",
    "                            data_aug=cfg.DATA.DATA_AUG,\n",
    "                            lazy_init=True)\n",
    "'''    \n",
    "root : str, required.\n",
    "    Path to the root folder storing the dataset.\n",
    "setting : str, required.\n",
    "    A text file describing the dataset, each line per video sample. There are three items in each line: (1) video path; (2) video length and (3) video label.\n",
    "train : bool, default True.\n",
    "    Whether to load the training or validation set.\n",
    "test_mode : bool, default False.\n",
    "    Whether to perform evaluation on the test set. Usually there is three-crop or ten-crop evaluation strategy involved.\n",
    "name_pattern : str, default None.\n",
    "    The naming pattern of the decoded video frames. For example, img_00012.jpg.\n",
    "video_ext : str, default 'mp4'.\n",
    "    If video_loader is set to True, please specify the video format accordinly.\n",
    "is_color : bool, default True.\n",
    "    Whether the loaded image is color or grayscale.\n",
    "modality : str, default 'rgb'.\n",
    "    Input modalities, we support only rgb video frames for now. Will add support for rgb difference image and optical flow image later.\n",
    "num_segments : int, default 1.\n",
    "    Number of segments to evenly divide the video into clips. A useful technique to obtain global video-level information.\n",
    "    Limin Wang, etal, Temporal Segment Networks: Towards Good Practices for Deep Action Recognition, ECCV 2016.\n",
    "num_crop : int, default 1.\n",
    "    Number of crops for each image. default is 1. Common choices are three crops and ten crops during evaluation.\n",
    "new_length : int, default 1.\n",
    "    The length of input video clip. Default is a single image, but it can be multiple video frames. For example, new_length=16 means we will extract a video clip of consecutive 16 frames.\n",
    "new_step : int, default 1.\n",
    "    Temporal sampling rate. For example, new_step=1 means we will extract a video clip of consecutive frames. new_step=2 means we will extract a video clip of every other frame.\n",
    "new_width : int, default 340.\n",
    "    Scale the width of loaded image to 'new_width' for later multiscale cropping and resizing.\n",
    "new_height : int, default 256.\n",
    "    Scale the height of loaded image to 'new_height' for later multiscale cropping and resizing.\n",
    "target_width : int, default 224.\n",
    "    Scale the width of transformed image to the same 'target_width' for batch forwarding.\n",
    "target_height : int, default 224.\n",
    "    Scale the height of transformed image to the same 'target_height' for batch forwarding.\n",
    "temporal_jitter : bool, default False.\n",
    "    Whether to temporally jitter if new_step > 1.\n",
    "video_loader : bool, default False.\n",
    "    Whether to use video loader to load data.\n",
    "use_decord : bool, default True.\n",
    "    Whether to use Decord video loader to load data. Otherwise use mmcv video loader.\n",
    "transform : function, default None.\n",
    "    A function that takes data and label and transforms them.\n",
    "slowfast : bool, default False.\n",
    "    If set to True, use data loader designed for SlowFast network. Christoph Feichtenhofer, etal, SlowFast Networks for Video Recognition, ICCV 2019.\n",
    "slow_temporal_stride : int, default 16.\n",
    "    The temporal stride for sparse sampling of video frames in slow branch of a SlowFast network.\n",
    "fast_temporal_stride : int, default 2.\n",
    "    The temporal stride for sparse sampling of video frames in fast branch of a SlowFast network.\n",
    "data_aug : str, default 'v1'.\n",
    "    Different types of data augmentation auto. Supports v1, v2, v3 and v4.\n",
    "lazy_init : bool, default False.\n",
    "    If set to True, build a dataset instance without loading any dataset.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _video_TSN_decord_batch_loader(directory, video_reader, duration, indices, skip_offsets):\n",
    "    skip_length = cfg.DATA.NEW_LENGTH * cfg.DATA.NEW_STEP ## needed (frames)\n",
    "    sampled_list = []\n",
    "    frame_id_list = []\n",
    "    \n",
    "    for seg_ind in indices:\n",
    "        offset = int(seg_ind)\n",
    "        for i, _ in enumerate(range(0, skip_length, cfg.DATA.NEW_STEP)):\n",
    "            if offset + skip_offsets[i] <= duration:\n",
    "                frame_id = offset + skip_offsets[i] - 1\n",
    "            else:\n",
    "                frame_id = offset - 1\n",
    "            frame_id_list.append(frame_id)\n",
    "            if offset + cfg.DATA.NEW_STEP < duration:\n",
    "                offset += cfg.DATA.NEW_STEP\n",
    "    try:\n",
    "        video_data = video_reader.get_batch(frame_id_list).asnumpy()\n",
    "        sampled_list = [video_data[vid, :, :, :] for vid, _ in enumerate(frame_id_list)]\n",
    "    except:\n",
    "        raise RuntimeError('Error occured in reading frames {} from video {} of duration {}.'.format(frame_id_list, directory, duration))\n",
    "    return sampled_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sample_test_indices(num_frames):\n",
    "    temporal_jitter = False\n",
    "    needed_frames = cfg.DATA.NEW_LENGTH * cfg.DATA.NEW_STEP\n",
    "\n",
    "    if num_frames > needed_frames - 1:\n",
    "        tick = (num_frames - needed_frames + 1) / \\\n",
    "            float(cfg.DATA.NUM_SEGMENTS)\n",
    "        print('num_frames: {}, tick: {}'.format(num_frames, tick))\n",
    "        offsets = np.array([int(tick / 2.0 + tick * x)\n",
    "                            for x in range(cfg.DATA.NUM_SEGMENTS)])\n",
    "    else:\n",
    "        offsets = np.zeros((cfg.DATA.NUM_SEGMENTS,))\n",
    "\n",
    "    if temporal_jitter:\n",
    "        skip_offsets = np.random.randint(\n",
    "            cfg.DATA.NEW_STEP, size=needed_frames // cfg.DATA.NEW_STEP)\n",
    "    else:\n",
    "        skip_offsets = np.zeros(\n",
    "            needed_frames // cfg.DATA.NEW_STEP, dtype=int)\n",
    "    return offsets + 1, skip_offsets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(cfg, video_name, transform, video_utils):\n",
    "\n",
    "    decord = try_import_decord()\n",
    "    decord_vr = decord.VideoReader(video_name, width=cfg.DATA.NEW_WIDTH, height=cfg.DATA.NEW_HEIGHT)\n",
    "    duration = len(decord_vr)\n",
    "    \n",
    "    \n",
    "    segment_indices, skip_offsets = video_utils._sample_test_indices(duration)\n",
    "    print(\"segment_indices\",segment_indices,\"skip_offsets\",skip_offsets,\"duration\",duration)\n",
    "    segment_indices2, skip_offsets2 = _sample_test_indices(duration)\n",
    "    assert np.allclose(segment_indices2 , segment_indices )\n",
    "\n",
    "\n",
    "    if cfg.DATA.SLOWFAST: \n",
    "        clip_input = video_utils._video_TSN_decord_slowfast_loader(video_name, decord_vr, duration, segment_indices, skip_offsets)\n",
    "    else: \n",
    "        clip_input = video_utils._video_TSN_decord_batch_loader(video_name, decord_vr, duration, segment_indices, skip_offsets)\n",
    "        #clip_input2 = _video_TSN_decord_batch_loader(video_name, decord_vr, duration, segment_indices, skip_offsets)\n",
    "        #np.allclose(clip_input2 , clip_input )\n",
    "    print(\"decord out\",np.shape(clip_input),type(clip_input))\n",
    "    \n",
    "    \n",
    "    viewer(clip_input)\n",
    "    \n",
    "\n",
    "    clip_input = transform(clip_input)\n",
    "    print(\"after transform\",np.shape(clip_input))\n",
    "    \n",
    "\n",
    "    if cfg.DATA.SLOWFAST:\n",
    "        sparse_sampels = len(clip_input) // (cfg.DATA.NUM_SEGMENTS * cfg.DATA.NUM_CROP)\n",
    "        clip_input = np.stack(clip_input, axis=0)\n",
    "        clip_input = clip_input.reshape((-1,) + (sparse_sampels, 3, cfg.TRANSFORM.INPUT_SIZE, cfg.TRANSFORM.INPUT_SIZE))\n",
    "        clip_input = np.transpose(clip_input, (0, 2, 1, 3, 4))\n",
    "    else:\n",
    "        clip_input = np.stack(clip_input, axis=0)\n",
    "        clip_input = clip_input.reshape((-1,) + (cfg.DATA.NEW_LENGTH, 3, cfg.TRANSFORM.INPUT_SIZE, cfg.TRANSFORM.INPUT_SIZE))\n",
    "        clip_input = np.transpose(clip_input, (0, 2, 1, 3, 4))\n",
    "\n",
    "    print(\"after reshape\",np.shape(clip_input))\n",
    "\n",
    "    if cfg.DATA.NEW_LENGTH == 1:\n",
    "        clip_input = np.squeeze(clip_input, axis=2)    # this is for 2D input case\n",
    "    \n",
    "    print(\"end\",np.shape(clip_input))\n",
    "    return nd.array(clip_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = list(range(0,1400,16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "for vid, vline in enumerate(data_list):\n",
    "    #if vid < 100 : continue\n",
    "    \n",
    "    video_path = vline.split()[0]\n",
    "    video_name = video_path.split('/')[-1]\n",
    "    video_data = read_data(cfg, video_path, transform_test, video_utils)\n",
    "    video_input = video_data.as_in_context(context)\n",
    "    video_feat = net(video_input.astype(cfg.MODEL.DTYPE, copy=False))\n",
    "\n",
    "    if cfg.DEBUG:\n",
    "        print(vid,'video_path', video_path)\n",
    "        print('video_data', video_data.shape)\n",
    "        print('video_input', video_input.shape)\n",
    "        print('video_feat', video_feat.shape,\"\\n                                        \")\n",
    "\n",
    "    #feat_file = '%s_%s_feat.npy' % (model_name, video_name)\n",
    "    #np.save(os.path.join(cfg.SAVE_DIR, feat_file), video_feat.asnumpy())\n",
    "\n",
    "    if vid > 0 and vid % cfg.LOG_INTERVAL == 0:\n",
    "        logger.info('%04d/%04d is done' % (vid, len(data_list)))\n",
    "    if vid == 1 : break\n",
    "    \n",
    "end_time = time.time()\n",
    "logger.info('Total feature extraction time is %4.2f minutes' % ((end_time - start_time) / 60))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SELF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 , numpy as np , os\n",
    "from gluoncv.utils.filesystem import try_import_decord\n",
    "from  utils.log import get_logger\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "decord = try_import_decord()\n",
    "def gerador_clips(video_path , cfg , transform , use_decord = True):\n",
    "\n",
    "    '''\n",
    "        gets batch_size of frames from video\n",
    "        yields chucnks of 16 frames\n",
    "        repeat until end\n",
    "    '''\n",
    "    \n",
    "    clip_length = cfg.DATA.NEW_LENGTH\n",
    "    if not use_decord:\n",
    "        vid = cv2.VideoCapture(video_path)\n",
    "        fps = int(vid.get(cv2.CAP_PROP_FPS))\n",
    "        tframes = int(vid.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        logger.info(f'{video_path}\\n   {str(tframes)} frames | {str(fps)} fps')\n",
    "        \n",
    "        frames = []\n",
    "        while True:\n",
    "            success, frame = vid.read()\n",
    "            if not success: break\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frames.append(frame)\n",
    "\n",
    "            if len(frames) >= clip_length:\n",
    "                \n",
    "                #if cfg.DATA.SLOWFAST:     \n",
    "                \n",
    "                #else:\n",
    "                frames = transform(frames) \n",
    "                frames = np.stack(frames, axis=0)\n",
    "                frames = frames.reshape((-1,) + (cfg.DATA.NEW_LENGTH, 3, cfg.TRANSFORM.INPUT_SIZE, cfg.TRANSFORM.INPUT_SIZE))\n",
    "                frames = np.transpose(frames, (0, 2, 1, 3, 4))    \n",
    "                yield frames\n",
    "                frames = []\n",
    "        vid.release()\n",
    "    \n",
    "    else:\n",
    "        vid = decord.VideoReader(video_path, width=cfg.DATA.NEW_WIDTH, height=cfg.DATA.NEW_HEIGHT)\n",
    "        duration = len(vid)\n",
    "        \n",
    "        idx_vid = list(range(0,duration,cfg.DATA.NEW_STEP))\n",
    "        segments = int(len(idx_vid) / cfg.DATA.NEW_LENGTH)\n",
    "        logger.info(f'yielding {segments} segments from {len(idx_vid)} frames ({str(int(duration/cfg.DATA.NEW_STEP))})')\n",
    "        yield segments\n",
    "        \n",
    "        for segment in range(segments):\n",
    "            idx_batch = idx_vid[segment*32:(segment+1)*32]\n",
    "            #logger.info(len(idx_batch),idx_batch)\n",
    "            frames = vid.get_batch(idx_batch).asnumpy()\n",
    "            frames = transform(frames) \n",
    "            frames = np.stack(frames, axis=0)\n",
    "            frames = frames.reshape((-1,) + (cfg.DATA.NEW_LENGTH, 3, cfg.TRANSFORM.INPUT_SIZE, cfg.TRANSFORM.INPUT_SIZE))\n",
    "            frames = np.transpose(frames, (0, 2, 1, 3, 4))    \n",
    "            yield frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.vid import gerador_clips\n",
    "import utils.list\n",
    "data_list = utils.list.get(cfg.DATA_LIST)\n",
    "\n",
    "vp = data_list[0].split()[0]\n",
    "clips = gerador_clips(vp , cfg , transform_test , True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zumx0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
