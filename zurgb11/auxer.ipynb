{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf 2.2.0\n",
      "\n",
      "Num GPUs Available:  4\n",
      "4 Physical GPUs, 4 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import os, time, random, logging , datetime , cv2 , csv , subprocess\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"tf\",tf.version.VERSION)\n",
    "from tensorflow import keras\n",
    "\n",
    "from utils import globo ,  xdv , tfh5\n",
    "\n",
    "\n",
    "''' GPU CONFIGURATION '''\n",
    "\n",
    "tfh5.set_tf_loglevel(logging.ERROR)\n",
    "tfh5.tf.debugging.set_log_device_placement(False) #Enabling device placement logging causes any Tensor allocations or operations to be printed.\n",
    "tfh5.set_memory_growth()\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/raid/DATASETS/anomaly/XD_Violence/training_copy_alter\n",
      "/raid/DATASETS/anomaly/XD_Violence/training_copy_alter/CUT\n",
      "\n",
      "full_train_fn (4013,) \n",
      "full_train_normal_fn (2045,) \n",
      "full_train_abnormal (1968,)\n",
      "\n",
      "train_fn (3210,) \n",
      "train_normal_fn (1618,) \n",
      "train_abnormal_fn (1592,)\n",
      "\n",
      "valdt_fn (803,) \n",
      "valdt_normal_fn (427,) \n",
      "valdt_abnormal_fn (376,)\n"
     ]
    }
   ],
   "source": [
    "''' TRAIN & VALDT '''\n",
    "#train_fn, train_labels, train_tot_frames, valdt_fn, valdt_labels , valdt_tot_frames = xdv.train_valdt_files(tframes=True)\n",
    "train_fn, train_labels, valdt_fn, valdt_labels = xdv.train_valdt_files()\n",
    "\n",
    "update_index_train = range(0, len(train_fn))\n",
    "update_index_valdt = range(0, len(valdt_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' CONFIGS '''\n",
    "\n",
    "train_config = {\n",
    "    \"frame_step\":2, #24 fps -> 12\n",
    "    \n",
    "    \"in_height\":120,\n",
    "    \"in_width\":160,\n",
    "    \n",
    "    \"batch_size\":1,\n",
    "    \"augment\":True,\n",
    "    \"shuffle\":False,\n",
    "    \n",
    "    \"ativa\" : 'leakyrelu',\n",
    "    \"optima\" : 'sgd',\n",
    "    \"batch_type\" : 0,   # =0 all batch have frame_max or video length // =1 last batch has frame_max frames // =2 last batch has no repetead frames\n",
    "    \"frame_max\" : 8000,\n",
    "    \"ckpt_start\" : f\"{0:0>8}\",  #used in train_model: if 00000000 start from scratch, else start from ckpt with config stated\n",
    "    \n",
    "    \"epochs\" : 1\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintDataCallback(tf.keras.callbacks.Callback):\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        # Access the input data shape of the current batch\n",
    "        input_batch_shape = self.model.get_layer('print_input_shape').output\n",
    "\n",
    "        # Run the model on a dummy input to get the actual input shape\n",
    "        input_shape = self.model.predict(np.zeros((1, *input_batch_shape.shape)))\n",
    "\n",
    "        # Print the input shape\n",
    "        print(\"Input shape:\\n\", input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGen(keras.utils.Sequence):\n",
    "    def __init__(self, vpath_list, label_list, config , valdt=False):\n",
    "        \n",
    "        self.valdt = valdt\n",
    "        self.vpath_list = vpath_list\n",
    "        self.label_list = label_list\n",
    "        \n",
    "        print(len(vpath_list),(len(label_list)))\n",
    "        \n",
    "        self.batch_size = config[\"batch_size\"]\n",
    "        self.frame_max = config[\"frame_max\"]\n",
    "        \n",
    "        self.in_height = config[\"in_height\"]\n",
    "        self.in_width = config[\"in_width\"]\n",
    "        \n",
    "        self.augment = config[\"augment\"]\n",
    "        self.shuffle = config[\"shuffle\"]\n",
    "        \n",
    "        self.len_vpath_list = len(self.vpath_list)\n",
    "        #self.indices = np.arange(self.len_vpath_list)\n",
    "\n",
    "        self.frame_step = config[\"frame_step\"]\n",
    "    \n",
    "    \n",
    "    def skip_ms(self,cap):\n",
    "        start_frame = cap.get(cv2.CAP_PROP_POS_FRAMES)\n",
    "        #print(\"skip_start\",start_frame)\n",
    "        while True:\n",
    "            success = cap.grab()\n",
    "            curr_frame = cap.get(cv2.CAP_PROP_POS_FRAMES)\n",
    "\n",
    "            if not success or curr_frame - start_frame >= self.frame_step:break\n",
    "        \n",
    "        if not success:return success, None, start_frame + self.frame_step\n",
    "\n",
    "        success, image = cap.retrieve()\n",
    "        return success, image, curr_frame        \n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        #if self.augment:\n",
    "        #    print(\"\\n\\n__len__ = n batchs = \",int(np.ceil(self.len_vpath_list * 2 / float(self.batch_size ))) ,\" w/ '2' vid each\")\n",
    "        #    return int(np.ceil(self.len_vpath_list * 2 / float(self.batch_size )))\n",
    "        #else:\n",
    "        #    print(\"\\n\\n__len__ = n batchs = \",int(np.ceil(self.len_vpath_list / float(self.batch_size))),\" w/ 1 vid each\")\n",
    "        #    return int(np.ceil(self.len_vpath_list / float(self.batch_size)))\n",
    "        return self.len_vpath_list\n",
    "           \n",
    "    def __getitem__(self, idx):\n",
    "        #batch_indices = self.indices[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "        print(\"\\n\\nbatch_indx\",idx)\n",
    "        \n",
    "        batch_frames , batch_frames_flip , batch_labels = [] , [] , []\n",
    "        \n",
    "        #for i, index in enumerate(batch_indices):\n",
    "        #vpath = self.vpath_list[index]\n",
    "        #label = self.label_list[index] \n",
    "        vpath = self.vpath_list[idx]\n",
    "        label = self.label_list[idx]\n",
    "    \n",
    "        video = cv2.VideoCapture(vpath)\n",
    "        tframes = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        print(\"\\n*************\",vpath , label , tframes)\n",
    "        \n",
    "        ## if normal > frame_max picks random frame_max W\n",
    "        if label == 0 and tframes > self.frame_max :\n",
    "            start_index = random.randint(0, tframes - self.frame_max)\n",
    "            end_index = start_index + self.frame_max\n",
    "            video.set(cv2.CAP_PROP_POS_FRAMES, start_index)\n",
    "        ## else ingests full video\n",
    "        else: \n",
    "            start_index = 0\n",
    "            end_index = tframes\n",
    "        \n",
    "        print(\"sstart_index,end_index\",start_index , end_index)\n",
    "        \n",
    "        frames = []\n",
    "        curr_frame = 0\n",
    "        success, frame = video.read()\n",
    "        for j in range(end_index - start_index):\n",
    "            \n",
    "            if not success or curr_frame > end_index: break\n",
    "            \n",
    "            frame = cv2.resize(frame, (self.in_width, self.in_height))\n",
    "            frame_arr = np.array(frame)/255.0\n",
    "            frames.append(frame_arr)\n",
    "            \n",
    "            ## jumps the next frame wo decoding\n",
    "            success, frame, curr_frame = self.skip_ms(video)\n",
    "            #print(\"skip_end\",curr_frame)\n",
    "                \n",
    "        \n",
    "        frames_arr = np.array(frames)\n",
    "        frames_arr_flip = np.flip(frames_arr, axis=2)\n",
    "        print(\"frames\",frames_arr.shape,frames_arr_flip.shape)\n",
    "\n",
    "        batch_frames.append(frames_arr)\n",
    "        batch_frames_flip.append(frames_arr_flip)\n",
    "        batch_labels.append(label)\n",
    "        \n",
    "        XN = np.array(batch_frames).astype(np.float32)\n",
    "        XF = np.array(batch_frames_flip).astype(np.float32)\n",
    "        y = np.array(batch_labels).astype(np.float32)\n",
    "        \n",
    "\n",
    "        if self.valdt or not self.augment:\n",
    "            print(\"valdt\")\n",
    "            print(\"XN \",XN.dtype,XN.shape )\n",
    "            print(\"y\",y.shape)\n",
    "            return XN , y\n",
    "        elif self.augment:\n",
    "            print(\"augment , train\")\n",
    "            X = np.concatenate([XN, XF], axis=0)\n",
    "            Y = np.concatenate([y, y], axis=0)\n",
    "            print(\"XN \",XN.dtype,XN.shape )\n",
    "            print(\"XF \",XF.dtype,XF.shape )\n",
    "            print(\"X \",X.dtype,X.shape )\n",
    "            print(\"Y \",Y.dtype,Y.shape)\n",
    "            return X , Y\n",
    "\n",
    "    #def on_epoch_end(self):\n",
    "    #    if self.shuffle:\n",
    "    #        np.random.shuffle(self.indexes)\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are only 8 videos being fed to the training phase, and batch_size is set to 1 with augment enabled, then the generator will yield 16 batches for each epoch of training, as each video will be flipped horizontally to create a second batch. This means that each video will be processed twice per epoch, once in its original orientation and once flipped horizontally.\n",
    "\n",
    "After all of the training batches have been processed, the fit method will move onto the validation data, which is processed separately using a different generator (valdt_generator).\n",
    "\n",
    "If augment is set to False, then each video will only yield one batch, regardless of the batch_size. So in this case, with a batch_size of 1, the generator would yield 8 batches for training before moving onto the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 8\n",
      "8 8\n",
      "\n",
      "FORM_MODEL\n",
      "\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_layer (InputLayer)        [(None, None, 120, 1 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_3 (Conv3D)               (None, None, 118, 15 220         input_layer[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3D)  (None, None, 59, 79, 0           conv3d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_4 (Conv3D)               (None, None, 57, 77, 1160        max_pooling3d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_4 (MaxPooling3D)  (None, None, 28, 38, 0           conv3d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_5 (Conv3D)               (None, None, 26, 36, 9232        max_pooling3d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_5 (MaxPooling3D)  (None, None, 13, 18, 0           conv3d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (1, None, 3744)      0           max_pooling3d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (1, None, 1024)      19533824    lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (1, 1024)            0           lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (1, 128)             131200      global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "output_layer (Dense)            (1, 1)               129         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "print_input_shape (Lambda)      (5,)                 0           input_layer[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 19,675,765\n",
      "Trainable params: 19,675,765\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "\t {'frame_step': 2, 'in_height': 120, 'in_width': 160, 'batch_size': 1, 'augment': True, 'shuffle': False, 'ativa': 'leakyrelu', 'optima': 'sgd', 'batch_type': 0, 'frame_max': 8000, 'ckpt_start': '00000000', 'epochs': 1} \n",
      "\n",
      "\tOPTIMA <tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f9ea2c65310> \n",
      "\tATIVA <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x7f9ea2e1f730>\n",
      "\n",
      "\n",
      "batch_indx 0\n",
      "\n",
      "************* /raid/DATASETS/anomaly/XD_Violence/training_copy_alter/v=CguBewD2LSQ__#1_label_A.mp4 0 4316\n",
      "sstart_index,end_index 0 4316\n",
      "frames (2158, 120, 160, 3) (2158, 120, 160, 3)\n",
      "augment , train\n",
      "XN  float32 (1, 2158, 120, 160, 3)\n",
      "XF  float32 (1, 2158, 120, 160, 3)\n",
      "X  float32 (2, 2158, 120, 160, 3)\n",
      "Y  float32 (2,)\n",
      "\n",
      "\n",
      "batch_indx 2\n",
      "\n",
      "************* /raid/DATASETS/anomaly/XD_Violence/training_copy_alter/Love.Actually.2003__#00-43-52_00-45-31_label_A.mp4 0 2376\n",
      "sstart_index,end_index 0 2376\n",
      "frames (1188, 120, 160, 3) (1188, 120, 160, 3)\n",
      "augment , train\n",
      "XN  float32 (1, 1188, 120, 160, 3)\n",
      "XF  float32 (1, 1188, 120, 160, 3)\n",
      "X  float32 (2, 1188, 120, 160, 3)\n",
      "Y  float32 (2,)\n",
      "\n",
      "\n",
      "batch_indx 4\n",
      "\n",
      "************* /raid/DATASETS/anomaly/XD_Violence/training_copy_alter/v=73uRcX0Dvfc__#00-03-12_00-04-15_label_B6-0-0.mp4 1 1514\n",
      "sstart_index,end_index 0 1514\n",
      "frames (757, 120, 160, 3) (757, 120, 160, 3)\n",
      "augment , train\n",
      "XN  float32 (1, 757, 120, 160, 3)\n",
      "XF  float32 (1, 757, 120, 160, 3)\n",
      "X  float32 (2, 757, 120, 160, 3)\n",
      "Y  float32 (2,)\n",
      "\n",
      "\n",
      "batch_indx 6\n",
      "\n",
      "************* /raid/DATASETS/anomaly/XD_Violence/training_copy_alter/v=9eME1y6V-T4__#01-42-00_01-48-00_label_A.mp4 0 8642\n",
      "sstart_index,end_index 259 8259\n",
      "frames (4000, 120, 160, 3) (4000, 120, 160, 3)\n",
      "augment , train\n",
      "XN  float32 (1, 4000, 120, 160, 3)\n",
      "XF  float32 (1, 4000, 120, 160, 3)\n",
      "X  float32 (2, 4000, 120, 160, 3)\n",
      "Y  float32 (2,)\n",
      "\n",
      "\n",
      "batch_indx 3\n",
      "\n",
      "************* /raid/DATASETS/anomaly/XD_Violence/training_copy_alter/Kingsman.The.Golden.Circle.2017__#02-00-02_02-01-21_label_B1-0-0.mp4 1 1898\n",
      "sstart_index,end_index 0 1898\n",
      "frames (949, 120, 160, 3) (949, 120, 160, 3)\n",
      "augment , train\n",
      "XN  float32 (1, 949, 120, 160, 3)\n",
      "XF  float32 (1, 949, 120, 160, 3)\n",
      "X  float32 (2, 949, 120, 160, 3)\n",
      "Y  float32 (2,)\n",
      "\n",
      "\n",
      "batch_indx 1\n",
      "\n",
      "************* /raid/DATASETS/anomaly/XD_Violence/training_copy_alter/v=o1UfPTzL_dM__#1_label_A.mp4 0 1907\n",
      "sstart_index,end_index 0 1907\n",
      "frames (954, 120, 160, 3) (954, 120, 160, 3)\n",
      "augment , train\n",
      "XN  float32 (1, 954, 120, 160, 3)\n",
      "XF  float32 (1, 954, 120, 160, 3)\n",
      "X  float32 (2, 954, 120, 160, 3)\n",
      "Y  float32 (2,)\n",
      "\n",
      "\n",
      "batch_indx 5\n",
      "\n",
      "************* /raid/DATASETS/anomaly/XD_Violence/training_copy_alter/Operation.Red.Sea.2018__#02-12-05_02-12-26_label_G-0-0.mp4 1 504\n",
      "sstart_index,end_index 0 504\n",
      "frames (252, 120, 160, 3) (252, 120, 160, 3)\n",
      "augment , train\n",
      "XN  float32 (1, 252, 120, 160, 3)\n",
      "XF  float32 (1, 252, 120, 160, 3)\n",
      "X  float32 (2, 252, 120, 160, 3)\n",
      "Y  float32 (2,)\n",
      "\n",
      "\n",
      "batch_indx 0\n",
      "\n",
      "************* /raid/DATASETS/anomaly/XD_Violence/training_copy_alter/v=CguBewD2LSQ__#1_label_A.mp4 0 4316\n",
      "sstart_index,end_index 0 4316\n",
      "frames (2158, 120, 160, 3) (2158, 120, 160, 3)\n",
      "augment , train\n",
      "XN  float32 (1, 2158, 120, 160, 3)\n",
      "XF  float32 (1, 2158, 120, 160, 3)\n",
      "X  float32 (2, 2158, 120, 160, 3)\n",
      "Y  float32 (2,)\n",
      "\n",
      "\n",
      "batch_indx 7\n",
      "\n",
      "************* /raid/DATASETS/anomaly/XD_Violence/training_copy_alter/v=5Rtsav021Rs__#00-00-00_00-03-00_label_A.mp4 0 4322\n",
      "sstart_index,end_index 0 4322\n",
      "frames (2161, 120, 160, 3) (2161, 120, 160, 3)\n",
      "augment , train\n",
      "XN  float32 (1, 2161, 120, 160, 3)\n",
      "XF  float32 (1, 2161, 120, 160, 3)\n",
      "X  float32 (2, 2161, 120, 160, 3)\n",
      "Y  float32 (2,)\n",
      "\n",
      "\n",
      "batch_indx 0\n",
      "\n",
      "************* /raid/DATASETS/anomaly/XD_Violence/training_copy_alter/Shoplifters.2018__#01-34-18_01-38-11_label_A.mp4 0 5593\n",
      "sstart_index,end_index 0 5593\n",
      "frames (2797, 120, 160, 3) (2797, 120, 160, 3)\n",
      "valdt\n",
      "XN  float32 (1, 2797, 120, 160, 3)\n",
      "y (1,)\n",
      "\n",
      "\n",
      "batch_indx 0\n",
      "\n",
      "************* /raid/DATASETS/anomaly/XD_Violence/training_copy_alter/Shoplifters.2018__#01-34-18_01-38-11_label_A.mp4 0 5593\n",
      "sstart_index,end_index 0 5593\n",
      "frames (2797, 120, 160, 3) (2797, 120, 160, 3)\n",
      "valdt\n",
      "XN  float32 (1, 2797, 120, 160, 3)\n",
      "y (1,)\n",
      "\n",
      "\n",
      "batch_indx 1\n",
      "\n",
      "************* /raid/DATASETS/anomaly/XD_Violence/training_copy_alter/Mindhunters.2004__#00-52-45_00-53-40_label_B2-0-0.mp4 1 1321\n",
      "sstart_index,end_index 0 1321\n",
      "frames (661, 120, 160, 3) (661, 120, 160, 3)\n",
      "valdt\n",
      "XN  float32 (1, 661, 120, 160, 3)\n",
      "y (1,)\n",
      "\n",
      "\n",
      "batch_indx 2\n",
      "\n",
      "************* /raid/DATASETS/anomaly/XD_Violence/training_copy_alter/Ip.Man.3.2015__#00-02-00_00-02-50_label_A.mp4 0 1200\n",
      "sstart_index,end_index 0 1200\n",
      "frames (600, 120, 160, 3) (600, 120, 160, 3)\n",
      "valdt\n",
      "XN  float32 (1, 600, 120, 160, 3)\n",
      "y (1,)\n",
      "\n",
      "\n",
      "batch_indx 3\n",
      "\n",
      "************* /raid/DATASETS/anomaly/XD_Violence/training_copy_alter/v=GZcVXqgQqx4__#1_label_A.mp4 0 3616\n",
      "sstart_index,end_index 0 3616\n",
      "frames (1808, 120, 160, 3) (1808, 120, 160, 3)\n",
      "valdt\n",
      "XN  float32 (1, 1808, 120, 160, 3)\n",
      "y (1,)\n",
      "\n",
      "\n",
      "batch_indx 4\n",
      "\n",
      "************* /raid/DATASETS/anomaly/XD_Violence/training_copy_alter/Braveheart.1995__#01-06-20_01-09-20_label_A.mp4 0 4321\n",
      "sstart_index,end_index 0 4321\n",
      "frames (2161, 120, 160, 3) (2161, 120, 160, 3)\n",
      "valdt\n",
      "XN  float32 (1, 2161, 120, 160, 3)\n",
      "y (1,)\n",
      "\n",
      "\n",
      "batch_indx 5\n",
      "\n",
      "************* /raid/DATASETS/anomaly/XD_Violence/training_copy_alter/Taken.Extended.Cut.2008__#01-09-33_01-09-50_label_B2-0-0.mp4 1 408\n",
      "sstart_index,end_index 0 408\n",
      "frames (204, 120, 160, 3) (204, 120, 160, 3)\n",
      "valdt\n",
      "XN  float32 (1, 204, 120, 160, 3)\n",
      "y (1,)\n",
      "\n",
      "\n",
      "batch_indx 6\n",
      "\n",
      "************* /raid/DATASETS/anomaly/XD_Violence/training_copy_alter/v=hOS0VtiFuUk__#1_label_B4-0-0.mp4 1 2890\n",
      "sstart_index,end_index 0 2890\n",
      "frames (1445, 120, 160, 3) (1445, 120, 160, 3)\n",
      "valdt\n",
      "XN  float32 (1, 1445, 120, 160, 3)\n",
      "y (1,)\n",
      "\n",
      "\n",
      "batch_indx 7\n",
      "\n",
      "************* /raid/DATASETS/anomaly/XD_Violence/training_copy_alter/Kingsman.The.Golden.Circle.2017__#00-50-38_00-51-42_label_A.mp4 0 1537\n",
      "sstart_index,end_index 0 1537\n",
      "frames (769, 120, 160, 3) (769, 120, 160, 3)\n",
      "valdt\n",
      "XN  float32 (1, 769, 120, 160, 3)\n",
      "y (1,)\n",
      "8/8 - 307s - loss: 0.7241 - output_layer_loss: 0.7241 - output_layer_tp: 6.0000 - output_layer_fp: 10.0000 - output_layer_tn: 0.0000e+00 - output_layer_fn: 0.0000e+00 - output_layer_accuracy: 0.3750 - output_layer_precision: 0.3750 - output_layer_recall: 1.0000 - output_layer_auc: 0.2333 - output_layer_prc: 0.2570 - val_loss: 0.6887 - val_output_layer_loss: 0.6887 - val_output_layer_tp: 3.0000 - val_output_layer_fp: 5.0000 - val_output_layer_tn: 0.0000e+00 - val_output_layer_fn: 0.0000e+00 - val_output_layer_accuracy: 0.3750 - val_output_layer_precision: 0.3750 - val_output_layer_recall: 1.0000 - val_output_layer_auc: 1.0000 - val_output_layer_prc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9ea2c82d60>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator = DataGen(train_fn[:8], train_labels[:8], train_config)\n",
    "\n",
    "valdt_generator = DataGen(valdt_fn[:8], valdt_labels[:8], train_config , True)\n",
    "\n",
    "## len(train_fn) / batch_size = number of video per batch = __len__\n",
    "## if batch_size 1 , each batch contains a video\n",
    "## if augmt =True & batch_size 1 , each batch contains \"2\" videos\n",
    "\n",
    "model = tfh5.form_model(train_config)\n",
    "\n",
    "model.fit(train_generator, \n",
    "          epochs = train_config[\"epochs\"] ,\n",
    "          steps_per_epoch = 8,#len(train_fn)*2,\n",
    "          \n",
    "          verbose=2,\n",
    "          \n",
    "          validation_data = valdt_generator ,\n",
    "          validation_steps = 8,#len(valdt_fn),\n",
    "          \n",
    "          #use_multiprocessing = True , \n",
    "          workers = 1 #,\n",
    "          #callbacks=[print_data_callback]\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NO CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_generator(video_paths, labels, fps, frame_max, in_height, in_width):\n",
    "    \n",
    "    \n",
    "    def skip_ms(cap,frame_step):\n",
    "        start_frame = cap.get(cv2.CAP_PROP_POS_FRAMES)\n",
    "        \n",
    "        while True:\n",
    "            success = cap.grab()\n",
    "\n",
    "            curr_frame = cap.get(cv2.CAP_PROP_POS_FRAMES)\n",
    "\n",
    "            if not success or curr_frame - start_frame >= frame_step:\n",
    "                break\n",
    "        \n",
    "        if not success:\n",
    "            return success, None, start_frame + frame_step\n",
    "\n",
    "        success, image = cap.retrieve()\n",
    "        \n",
    "        return success, image, curr_frame    \n",
    "    \n",
    "    \n",
    "    #while True:\n",
    "    # Shuffle the video paths and labels\n",
    "    zipped = list(zip(video_paths, labels))\n",
    "    random.shuffle(zipped)\n",
    "    video_paths, labels = zip(*zipped)\n",
    "    \n",
    "    # Loop over the video paths and labels\n",
    "    #for i in range(len(video_paths)):\n",
    "    video_path = video_paths[0]\n",
    "    label = labels[0]\n",
    "    \n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    tframes = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    print(video_path , label , tframes)\n",
    "    \n",
    "    if label == 0 and tframes > frame_max :\n",
    "        start_index = random.randint(0, tframes - frame_max)\n",
    "        end_index = start_index + frame_max\n",
    "        video.set(cv2.CAP_PROP_POS_FRAMES, start_index)\n",
    "    else : \n",
    "        start_index = 0\n",
    "        end_index = tframes\n",
    "    \n",
    "    print(start_index , end_index)\n",
    "    \n",
    "    frames = []\n",
    "    curr_frame = 0\n",
    "    \n",
    "    success, frame = video.read()\n",
    "    while 1:\n",
    "        \n",
    "        if not success or curr_frame > end_index: break\n",
    "        \n",
    "        frame = cv2.resize(frame, (in_width, in_height))\n",
    "        frame = np.array(frame)/255.0\n",
    "        frames.append(frame)\n",
    "        \n",
    "        '''cv2.imshow('ff', frame)\n",
    "        key = cv2.waitKey(int(1000/12))\n",
    "        if key == ord('q'): break  # quit\n",
    "        if key == ord(' '):  # pause\n",
    "            while True:\n",
    "                key = cv2.waitKey(1)\n",
    "                if key == ord(' '):break'''\n",
    "                \n",
    "        success, frame, curr_frame = skip_ms(video,2)\n",
    "        \n",
    "    #batch_frames.append(frames)\n",
    "    #batch_labels.append(label)\n",
    "    #\n",
    "    #batch_frames = np.array(batch_frames) / 255.0\n",
    "    #batch_labels = np.array(batch_labels)\n",
    "    \n",
    "    X = np.array(frames)\n",
    "    \n",
    "        \n",
    "    return np.expand_dims(X,0) , np.array([label])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x , y = video_generator(train_fn,train_labels,12,8000,120,160)\n",
    "\n",
    "#print( np.shape(x) , np.shape(y))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ORIGINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" INPUT DATA\"\"\"\n",
    "\n",
    "in_height = 120; in_width = 160\n",
    "\n",
    "def input_train_video_data(file_name):\n",
    "    print(\"\\n\\ninput_train_video_data\\n\")\n",
    "    \n",
    "    #file_name = 'C:\\\\Bosch\\\\Anomaly\\\\training\\\\videos\\\\13_007.avi'\n",
    "    \n",
    "    video = cv2.VideoCapture(file_name)\n",
    "    total_frame = video.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    \n",
    "    divid_no = 1\n",
    "    \n",
    "    frame_max = train_config[\"frame_max\"]\n",
    "    \n",
    "    # define the nmbers of batchs to divid atual video (divid_no)\n",
    "    if total_frame > int(frame_max):\n",
    "        total_frame_int = int(total_frame)\n",
    "        if total_frame_int % int(frame_max) == 0:\n",
    "            divid_no = int(total_frame / int(frame_max))\n",
    "        else:\n",
    "            divid_no = int(total_frame / int(frame_max)) + 1\n",
    "        \n",
    "    batch_no = 0\n",
    "    batch_frames = []\n",
    "    batch_frames_flip = []\n",
    "    counter = 0\n",
    "    \n",
    "    # gets random batch w\\ frame max lenght \n",
    "    if 'Normal' in file_name:\n",
    "        print(\"\\n\\nNORMAL\\n\\n\")\n",
    "        if divid_no != 1:\n",
    "            slice_no = int(random.random()*divid_no)\n",
    "            passby = 0\n",
    "            if slice_no != divid_no - 1:\n",
    "                while video.isOpened and passby < int(frame_max) * slice_no:\n",
    "                    passby += 1\n",
    "                    success, image = video.read()\n",
    "                    if success == False:\n",
    "                        break\n",
    "            else:\n",
    "                while video.isOpened and passby < total_frame - int(frame_max):\n",
    "                    passby += 1\n",
    "                    success, image = video.read()\n",
    "                    if success == False:\n",
    "                        break\n",
    "\n",
    "    while video.isOpened:               \n",
    "        success, image = video.read()\n",
    "        if success == False:\n",
    "            break\n",
    "\n",
    "        image = cv2.resize(image, (in_width, in_height))\n",
    "        image_flip = cv2.flip(image, 1)\n",
    "        \n",
    "        image_array = np.array(image)/255.0\n",
    "        image_array_flip = np.array(image_flip)/255.0\n",
    "        \n",
    "        batch_frames.append(image_array)\n",
    "        batch_frames_flip.append(image_array_flip)\n",
    "        \n",
    "        counter += 1\n",
    "        if counter > int(frame_max):\n",
    "            break\n",
    "            \n",
    "    video.release()\n",
    "    batch_frames = np.array(batch_frames)\n",
    "    print(batch_frames.shape)\n",
    "        \n",
    "    return np.expand_dims(batch_frames,0), np.expand_dims(batch_frames_flip, 0), total_frame\n",
    "\n",
    "\n",
    "\n",
    "def generate_input(data,update_index,validation):\n",
    "    \n",
    "    #data_var_name = [k for k, v in globals().items() if v is data][0]\n",
    "    #print(\"\\n\\nGENERATE_INPUT FOR\",data_var_name,\\\n",
    "    #    '\\n\\tupdate_index len = ',len(update_index),\\\n",
    "    #    '\\n\\tdata len = ',len(data))\n",
    "    \n",
    "    loop_no = 0\n",
    "    while 1:\n",
    "        index = update_index[loop_no]\n",
    "        loop_no += 1\n",
    "        #print(\"\\n\",data_var_name,\" index\",index,\" loop_no\",loop_no)\n",
    "        if loop_no == len(data):loop_no= 0\n",
    "        \n",
    "        \n",
    "        batch_frames, batch_frames_flip, total_frames = input_train_video_data(data[index])\n",
    "        print(\"\\n\\tdata[\",index,\"]=\",data[index],\"\\n\\ttotal_frames=\",total_frames,\"\\n\\tbatch_frames.shape=\",batch_frames.shape,\"\\n\")\n",
    "        #if batch_frames.ndim != 5:\n",
    "        #   break\n",
    "\n",
    "        \n",
    "        if not validation:\n",
    "            #batch_frames\n",
    "            if 'label_A' in data[index]: return batch_frames, np.array([0])   #normal\n",
    "            else: return batch_frames, np.array([1])   #abnormal\n",
    "            \n",
    "            #batch_frames_flip\n",
    "            if 'label_A' in data[index]: return batch_frames_flip, np.array([0])  #normal\n",
    "            else: return batch_frames_flip, np.array([1])  #abnormal\n",
    "        else:\n",
    "            #batch_frames\n",
    "            if 'label_A' in data[index]: return batch_frames, np.array([0])   #normal\n",
    "            else: return batch_frames, np.array([1])   #abnormal\n",
    "                \n",
    "    print(\"\\nloop_no=\",loop_no)\n",
    "    \n",
    "    \n",
    "#generate_input(train_fn[:4] , update_index_train[:4] , False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zugpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
