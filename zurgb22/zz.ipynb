{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "class VideoFeatureExtractor:\n",
    "\n",
    "    def __init__(self, model, resolution=224, clip_length=32, stride=32):\n",
    "        self.model = model\n",
    "        self.resolution = resolution\n",
    "        self.clip_length = clip_length\n",
    "        self.stride = stride\n",
    "\n",
    "    def sliding_window(self, arr):\n",
    "        size = self.clip_length\n",
    "        stride = self.stride\n",
    "        num_chunks = int((len(arr) - size) / stride) + 2\n",
    "        result = []\n",
    "        for i in range(0, num_chunks * stride, stride):\n",
    "            if len(arr[i:i + size]) > 0:\n",
    "                result.append(arr[i:i + size])\n",
    "                \n",
    "        if len(result) % self.clip_length != 0: result = result[:-1]\n",
    "        print(\"\\tSLIDED IN2\",np.shape(result)[0],np.shape(result)[1:])\n",
    "        return np.array(result)\n",
    "\n",
    "    def get_video_clips(self, video_path):\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        while (cap.isOpened()):\n",
    "            sucess, frame = cap.read()\n",
    "            if not sucess: break\n",
    "            frame = cv2.resize(frame, (self.resolution,self.resolution))\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frames.append(frame)\n",
    "        cap.release()\n",
    "        print(\"\\tFRAMED\",np.shape(frames)[0],np.shape(frames)[1:])\n",
    "\n",
    "        # Create clips using sliding window\n",
    "        clips = self.sliding_window(frames)\n",
    "        return clips\n",
    "\n",
    "    def normalize_input(self, batch_clips):\n",
    "        mean = np.array([0.485 * 255, 0.456 * 255, 0.406 * 255], dtype=np.float32)\n",
    "        std = np.array([0.229 * 255, 0.224 * 255, 0.225 * 255], dtype=np.float32)\n",
    "        return (batch_clips - mean) / std\n",
    "\n",
    "    def extract_features_per_clip(self, video_path):\n",
    "        # Load video clips\n",
    "        clips = self.get_video_clips(video_path)\n",
    "\n",
    "        # Preprocess and extract features\n",
    "        features = []\n",
    "        for clip in clips:\n",
    "            input_batch = clip.astype(np.float32)  # Keep pixel values in the [0, 255] range\n",
    "            input_batch = self.normalize_input(input_batch)  # Normalize input using ImageNet mean and std values\n",
    "            input_batch = np.expand_dims(input_batch, axis=0)  # Add batch dimension\n",
    "            input_batch = np.transpose(input_batch, (0, 4, 1, 2, 3))  # Rearrange to [batch_size, channels, frames, height, width]\n",
    "            feature = self.model(input_batch)\n",
    "            features.append(feature.numpy())\n",
    "\n",
    "        return np.array(features)\n",
    "\n",
    "    def extract_features_per_video(self, video_path):\n",
    "        clip_features = self.extract_features_per_clip(video_path)\n",
    "        video_features = np.mean(clip_features, axis=0)\n",
    "        return video_features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEATURE EXTRACTOR USING VSWIN\n",
    "- LOAD XDV TRAIN\n",
    "- GET EACH VIDEO INTO CLIPS OF clips_length frames\n",
    "- EXTRACT FEATURES WITH VSWIN TRANFORMER\n",
    "- THEN TRY TO INTERPOLATE FEATURRES USING THE SULTANI METHODS SO FEATURES ARE ALL SAME LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/raid/DATASETS/.zuble/vigia /raid/DATASETS/anomaly\n",
      "\n",
      "LOADING train data (3210,) (3210,) (3210,) \n",
      "\n",
      "\n",
      "\tnormal 1618\n",
      "\tabnormal 1592\n"
     ]
    }
   ],
   "source": [
    "from utils import globo , xdv\n",
    "import cv2 , os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "fn , labels , tframes = xdv.load_train_valdt_npy('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.saved_model.load(\"/raid/DATASETS/.zuble/vigia/zurgb11/.pretrained/swin_tiny_patch244_window877_kinetics400_1k_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = 224\n",
    "channels = 3\n",
    "clip_length = 32\n",
    "features_per_bag = 32\n",
    "\n",
    "def get_video_clips(video_path, clip_length = clip_length , stride = clip_length):\n",
    "    \n",
    "    def sliding_window(arr, size, stride):\n",
    "        num_chunks = int((len(arr) - size) / stride) + 2\n",
    "        result = []\n",
    "        for i in range(0,  num_chunks * stride, stride):\n",
    "            if len(arr[i:i + size]) > 0:\n",
    "                #print(i,len(arr[i:i + size]))\n",
    "                result.append(arr[i:i + size])\n",
    "                \n",
    "        # Remove last clip if number of frames is not equal to 32\n",
    "        if len(result) % clip_length != 0: result = result[:-1]\n",
    "        print(\"\\tSLIDED IN2\",np.shape(result)[0],np.shape(result)[1:])\n",
    "        return np.array(result)   \n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while (cap.isOpened()):\n",
    "        sucess, frame = cap.read()\n",
    "        if not sucess: break\n",
    "        frame = cv2.resize(frame, (resolution,resolution))\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    print(\"\\tFRAMED\",np.shape(frames)[0],np.shape(frames)[1:])\n",
    "    \n",
    "    return sliding_window(frames, clip_length, stride)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 Kingsman.The.Golden.Circle.2017__#02-00-02_02-01-21_label_B1-0-0.mp4\n",
      "\tlabel 1\n",
      "\t1898 frames\n"
     ]
    }
   ],
   "source": [
    "for i,tframe in enumerate(tframes):\n",
    "    if tframe > 1000 and labels[i] == 1:\n",
    "        print(f'{i} {os.path.basename(fn[i])}\\n\\tlabel {labels[i]}\\n\\t{tframes[i]} frames')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFRAMED 1898 (224, 224, 3)\n",
      "\tSLIDED IN2 59 (32, 224, 224, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(59, 32, 224, 224, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clips = get_video_clips(fn[3])\n",
    "clips.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_input(batch_clips):\n",
    "    mean = np.array([0.485 * 255, 0.456 * 255, 0.406 * 255], dtype=np.float32)\n",
    "    std = np.array([0.229 * 255, 0.224 * 255, 0.225 * 255], dtype=np.float32)\n",
    "    return (batch_clips - mean) / std\n",
    "\n",
    "# Preprocess and extract features\n",
    "features = []\n",
    "for clip in clips:\n",
    "    input_batch = clip.astype(np.float32)  # Keep pixel values in the [0, 255] range\n",
    "    input_batch = normalize_input(input_batch)  # Normalize input using ImageNet mean and std values\n",
    "    input_batch = np.expand_dims(input_batch, axis=0)  # Add batch dimension\n",
    "    input_batch = np.transpose(input_batch, (0, 4, 1, 2, 3))  # Rearrange to [batch_size, channels, frames, height, width]\n",
    "    feature = model(input_batch)[0].numpy()\n",
    "    features.append(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 768, 16, 7, 7)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## AnomalyDetection_CVPR18\n",
    "def fsegmentation_sultani(features, features_per_bag):\n",
    "    feature_size = np.array(features).shape[1]\n",
    "    interpolated_features = np.zeros((features_per_bag, feature_size))\n",
    "    interpolation_indicies = np.round(np.linspace(0, len(features) - 1, num=features_per_bag + 1))\n",
    "    count = 0\n",
    "    for index in range(0, len(interpolation_indicies)-1):\n",
    "        print(\"interpolate\",index)\n",
    "        start = int(interpolation_indicies[index])\n",
    "        end = int(interpolation_indicies[index + 1])\n",
    "\n",
    "        assert end >= start\n",
    "\n",
    "        if start == end: temp_vect = features[start, :]\n",
    "        else: temp_vect = np.mean(features[start:end+1, :], axis=0)\n",
    "\n",
    "        temp_vect = temp_vect / np.linalg.norm(temp_vect)\n",
    "\n",
    "        if np.linalg.norm(temp_vect) == 0: print(\"Error\")\n",
    "\n",
    "        interpolated_features[count,:]=temp_vect\n",
    "        count = count + 1\n",
    "    return np.array(interpolated_features)\n",
    "\n",
    "## tfm-anomaly-detection\n",
    "def fsegmentation_tfm(features, n_segments=32):\n",
    "    import sklearn.preprocessing\n",
    "    if features.shape[0] < n_segments:\n",
    "        raise RuntimeError(\"Number of prev segments lesser than expected output size\")\n",
    "\n",
    "    cuts = np.linspace(0, features.shape[0], n_segments, dtype=int, endpoint=False)\n",
    "\n",
    "    new_feats = []\n",
    "    for i, j in zip(cuts[:-1], cuts[1:]):\n",
    "        new_feats.append(np.mean(features[i:j,:], axis=0))\n",
    "\n",
    "    new_feats.append(np.mean(features[cuts[-1]:,:], axis=0))\n",
    "\n",
    "    new_feats = np.array(new_feats)\n",
    "    new_feats = sklearn.preprocessing.normalize(new_feats, axis=1)\n",
    "    return new_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_per_clip(video_path, model):\n",
    "    # Load video clips\n",
    "    clips = get_video_clips(video_path)\n",
    "\n",
    "    features = []\n",
    "    for clip in clips:\n",
    "        input_batch = clip.astype(np.float32)  # Keep pixel values in the [0, 255] range\n",
    "        input_batch = normalize_input(input_batch)  # Normalize input using ImageNet mean and std values\n",
    "        input_batch = np.expand_dims(input_batch, axis=0)  # Add batch dimension\n",
    "        feature = model(input_batch)\n",
    "        features.append(feature.numpy())\n",
    "\n",
    "    return np.array(features)\n",
    "\n",
    "def extract_features_per_video(video_path, model, spatial_size=224):\n",
    "    clip_features = extract_features_per_clip(video_path, model, spatial_size)\n",
    "    video_features = np.mean(clip_features, axis=0)\n",
    "    return video_features\n",
    "\n",
    "model = tf.saved_model.load(\"/raid/DATASETS/.zuble/vigia/zurgb11/.pretrained/swin_tiny_patch244_window877_kinetics400_1k_1\")\n",
    "\n",
    "vf = extract_features_per_video(fn[1] , model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zugpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
